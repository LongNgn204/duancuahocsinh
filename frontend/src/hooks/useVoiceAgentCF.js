// src/hooks/useVoiceAgentCF.js
// ChÃº thÃ­ch: Hook Voice Chat vá»›i Gemini AI
// STT: SpeechRecognition (vi-VN) - cháº¡y trÃªn browser
// TTS: Gemini TTS (gemini-2.5-pro-preview-tts) vá»›i fallback browser
// LLM: Gemini API frontend streaming
// SOS: PhÃ¡t hiá»‡n tá»« khÃ³a tiÃªu cá»±c vÃ  hiá»‡n cáº£nh bÃ¡o

// v9.0: Chuyá»ƒn sang Gemini TTS vá»›i fallback browser SpeechSynthesis
import { useState, useCallback, useRef, useEffect } from 'react';
import { detectSOSLevel, sosMessage, getSuggestedAction } from '../utils/sosDetector';
import { streamChat, isGeminiConfigured } from '../services/gemini';
import { speak as geminiSpeak, stopSpeaking as geminiStop } from '../services/geminiTTS';

/**
 * Loáº¡i bá» emoji vÃ  icon khá»i text trÆ°á»›c khi TTS Ä‘á»c
 * @param {string} text - Text cÃ³ thá»ƒ chá»©a emoji
 * @returns {string} Text Ä‘Ã£ lá»c bá» emoji
 */
function stripEmoji(text) {
    if (!text) return '';
    // Regex loáº¡i bá» emoji, symbol, pictograph
    return text
        .replace(/[\u{1F600}-\u{1F64F}]/gu, '') // Emoticons
        .replace(/[\u{1F300}-\u{1F5FF}]/gu, '') // Misc Symbols and Pictographs
        .replace(/[\u{1F680}-\u{1F6FF}]/gu, '') // Transport and Map
        .replace(/[\u{1F1E0}-\u{1F1FF}]/gu, '') // Flags
        .replace(/[\u{2600}-\u{26FF}]/gu, '')   // Misc Symbols
        .replace(/[\u{2700}-\u{27BF}]/gu, '')   // Dingbats
        .replace(/[\u{FE00}-\u{FE0F}]/gu, '')   // Variation Selectors
        .replace(/[\u{1F900}-\u{1F9FF}]/gu, '') // Supplemental Symbols
        .replace(/[\u{1FA00}-\u{1FA6F}]/gu, '') // Chess symbols, etc.
        .replace(/[\u{1FA70}-\u{1FAFF}]/gu, '') // Symbols and Pictographs Extended-A
        .replace(/[\u{231A}-\u{2B55}]/gu, '')   // Misc symbols
        .replace(/[\u{23E9}-\u{23F3}]/gu, '')   // Symbols
        .replace(/[\u{23F8}-\u{23FA}]/gu, '')   // Symbols
        .replace(/[\u{25AA}-\u{25FE}]/gu, '')   // Shapes
        .replace(/ðŸ†˜|ðŸ“ž|ðŸŒ™|ðŸ’ª|ðŸŽ®|ðŸ§˜|ðŸ“–|âœ¨|ðŸŒŸ|â­|ðŸ’¬|ðŸ¤–|ðŸŽ¯|ðŸ’¡|â¤ï¸|ðŸ’š|ðŸ’™|ðŸ”µ|ðŸ”´|ðŸŸ¢|ðŸŸ¡|âš ï¸|âœ…|âŒ|ðŸ”¥|ðŸ‘‹|ðŸ‘|ðŸ‘Ž|ðŸ™|ðŸ’•|ðŸŒˆ|â˜€ï¸|ðŸŒ™|â°|ðŸ“|ðŸ“Š|ðŸ†|ðŸŽ‰|ðŸ˜Š|ðŸ˜¢|ðŸ˜¤|ðŸ˜|ðŸŒ¸/g, '')
        .replace(/\s{2,}/g, ' ') // Multiple spaces to single
        .trim();
}

/**
 * Hook Voice Agent vá»›i Web Speech API
 * @returns {Object} Voice agent state vÃ  controls
 */
/**
 * Hook Voice Agent vá»›i Web Speech API
 * @param {Object} options - Configuration options
 * @param {Function} options.onSOS - Callback when SOS detected (level, message)
 * @returns {Object} Voice agent state vÃ  controls
 */
export function useVoiceAgentCF(options = {}) {
    const { onSOS, autoSubmit = true, onResult } = options;
    // ========================================================================
    // STATE
    // ========================================================================
    const [status, setStatus] = useState('idle'); // idle | listening | thinking | speaking
    const [transcript, setTranscript] = useState('');
    const [response, setResponse] = useState('');
    const [error, setError] = useState(null);
    const [isSupported, setIsSupported] = useState(true);
    const [sosDetected, setSosDetected] = useState(null); // { level, message }

    // Refs
    const recognitionRef = useRef(null);
    const abortControllerRef = useRef(null);
    const isSpeakingRef = useRef(false);

    // ========================================================================
    // CHECK BROWSER SUPPORT
    // ========================================================================
    useEffect(() => {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

        if (!SpeechRecognition) {
            setIsSupported(false);
            console.error('[VoiceAgent] Browser not supported - SpeechRecognition required');
            setError('TrÃ¬nh duyá»‡t cá»§a báº¡n khÃ´ng há»— trá»£ nháº­n diá»‡n giá»ng nÃ³i. Vui lÃ²ng sá»­ dá»¥ng Google Chrome hoáº·c Edge.');
        }
    }, []);

    // ========================================================================
    // SPEECH RECOGNITION (STT)
    // ========================================================================
    const startListening = useCallback(() => {
        if (!isSupported) return;

        setError(null);
        setTranscript('');

        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        const recognition = new SpeechRecognition();

        // Cáº¥u hÃ¬nh STT cho tiáº¿ng Viá»‡t
        recognition.lang = 'vi-VN';
        recognition.continuous = false;      // Dá»«ng sau khi nghe xong 1 cÃ¢u
        recognition.interimResults = true;   // Hiá»ƒn thá»‹ káº¿t quáº£ táº¡m
        recognition.maxAlternatives = 1;

        recognition.onstart = () => {
            console.log('[VoiceAgent] STT started');
            setStatus('listening');
        };

        recognition.onresult = (event) => {
            let interimTranscript = '';
            let finalTranscript = '';

            for (let i = event.resultIndex; i < event.results.length; i++) {
                const result = event.results[i];
                if (result.isFinal) {
                    finalTranscript += result[0].transcript;
                } else {
                    interimTranscript += result[0].transcript;
                }
            }

            // Hiá»ƒn thá»‹ transcript (interim hoáº·c final)
            const currentText = finalTranscript || interimTranscript;
            setTranscript(currentText);

            // Náº¿u cÃ³ final transcript
            if (finalTranscript) {
                console.log('[VoiceAgent] Final transcript:', finalTranscript);

                // Make autoSubmit optional
                if (autoSubmit !== false) {
                    sendToLLM(finalTranscript);
                }

                // Allow external handling
                if (onResult) {
                    onResult(finalTranscript);
                }
            }
        };

        recognition.onerror = (event) => {
            console.error('[VoiceAgent] STT error:', event.error);
            if (event.error === 'no-speech') {
                setError('KhÃ´ng nghe tháº¥y gÃ¬. Báº¡n hÃ£y nÃ³i to hÆ¡n nhÃ©.');
            } else if (event.error === 'not-allowed') {
                setError('Vui lÃ²ng cho phÃ©p Micro Ä‘á»ƒ trÃ² chuyá»‡n vá»›i mÃ¬nh nhÃ©!');
            } else {
                setError(`Lá»—i micrÃ´: ${event.error}. HÃ£y thá»­ táº£i láº¡i trang.`);
            }
            setStatus('idle');
        };

        recognition.onend = () => {
            console.log('[VoiceAgent] STT ended');
            recognitionRef.current = null;
            // KhÃ´ng reset status á»Ÿ Ä‘Ã¢y vÃ¬ cÃ³ thá»ƒ Ä‘ang thinking/speaking
        };

        recognitionRef.current = recognition;
        recognition.start();
    }, [isSupported]);

    const stopListening = useCallback(() => {
        if (recognitionRef.current) {
            recognitionRef.current.stop();
            recognitionRef.current = null;
        }
        setStatus('idle');
    }, []);

    // ========================================================================
    // LLM CALL (Gemini Streaming)
    // ========================================================================
    const sendToLLM = useCallback(async (text) => {
        // ====== SOS DETECTION - QUAN TRá»ŒNG ======
        const sosLevel = detectSOSLevel(text);
        const sosAction = getSuggestedAction(sosLevel);

        if (sosAction.showOverlay) {
            const msg = sosMessage(sosLevel);
            setSosDetected({ level: sosLevel, message: msg });

            // Gá»i callback Ä‘á»ƒ hiá»ƒn thá»‹ overlay
            if (onSOS) {
                onSOS(sosLevel, msg);
            }

            // Náº¿u critical, block hoÃ n toÃ n
            if (sosAction.blockNormalResponse) {
                console.log('[VoiceAgent] SOS CRITICAL detected, blocking response');
                // Äá»c thÃ´ng Ä‘iá»‡p SOS thay vÃ¬ gá»i LLM
                speak(stripEmoji(msg));
                return;
            }
        }
        // ====== END SOS DETECTION ======

        console.log('[VoiceAgent] Sending to LLM:', text);
        setStatus('thinking');
        setResponse('');

        try {
            let fullResponse = '';

            console.log('[VoiceAgent] Calling streamChat...');
            // Gá»i Gemini Streaming tá»« service
            await streamChat(
                text,
                [],
                (chunk) => {
                    fullResponse += chunk;
                    setResponse(fullResponse);
                }
            );

            console.log('[VoiceAgent] Gemini Response complete:', fullResponse.length, 'chars');

            // Respond
            if (fullResponse) {
                console.log('[VoiceAgent] Speaking response...');
                speak(fullResponse);
            } else {
                console.warn('[VoiceAgent] Empty response from Gemini');
                setStatus('idle');
            }

        } catch (err) {
            console.error('[VoiceAgent] Gemini error:', err);
            const errorMsg = isGeminiConfigured()
                ? 'Xin lá»—i, mÃ¬nh Ä‘ang gáº·p chÃºt trá»¥c tráº·c. Báº¡n nÃ³i láº¡i Ä‘Æ°á»£c khÃ´ng?'
                : 'ChÆ°a cáº¥u hÃ¬nh API Key. Vui lÃ²ng kiá»ƒm tra cÃ i Ä‘áº·t.';

            setError(errorMsg);
            speak(errorMsg);
            setStatus('idle');
        }
    }, []);

    // ========================================================================
    // SPEECH SYNTHESIS (TTS) - Gemini TTS with browser fallback
    // ========================================================================
    const speak = useCallback(async (text) => {
        if (!text) {
            setStatus('idle');
            return;
        }

        setStatus('speaking');
        isSpeakingRef.current = true;

        // FILTER EMOJI TRÆ¯á»šC KHI Äá»ŒC
        const cleanText = stripEmoji(text);
        if (!cleanText) {
            console.log('[VoiceAgent] No text to speak after emoji filter');
            setStatus('idle');
            isSpeakingRef.current = false;
            return;
        }

        try {
            await geminiSpeak(cleanText, {
                fallbackToBrowser: true,
                onEnd: () => {
                    console.log('[VoiceAgent] TTS ended');
                    setStatus('idle');
                    isSpeakingRef.current = false;
                },
                onError: (e) => {
                    console.error('[VoiceAgent] TTS error:', e);
                    setStatus('idle');
                    isSpeakingRef.current = false;
                }
            });
        } catch (error) {
            console.error('[VoiceAgent] TTS failed:', error);
            setStatus('idle');
            isSpeakingRef.current = false;
        }
    }, []);

    const stopSpeaking = useCallback(() => {
        geminiStop();
        isSpeakingRef.current = false;
        setStatus('idle');
    }, []);

    // ========================================================================
    // STOP ALL
    // ========================================================================
    const stop = useCallback(() => {
        // Stop STT
        if (recognitionRef.current) {
            recognitionRef.current.stop();
            recognitionRef.current = null;
        }

        // Abort LLM request
        if (abortControllerRef.current) {
            abortControllerRef.current.abort();
            abortControllerRef.current = null;
        }

        // Stop TTS (Gemini + browser)
        geminiStop();
        isSpeakingRef.current = false;

        setStatus('idle');
        setError(null);
    }, []);

    // ========================================================================
    // CLEANUP
    // ========================================================================
    useEffect(() => {
        return () => {
            stop();
        };
    }, [stop]);

    // ========================================================================
    // RETURN
    // ========================================================================
    // Clear SOS state
    const clearSOS = useCallback(() => {
        setSosDetected(null);
    }, []);

    return {
        // State
        status,           // 'idle' | 'listening' | 'thinking' | 'speaking'
        transcript,       // Current STT transcript
        response,         // Current LLM response
        error,            // Error message if any
        isSupported,      // Browser support check
        sosDetected,      // { level, message } if SOS detected

        // Controls
        startListening,   // Start voice input
        stopListening,    // Stop voice input
        stopSpeaking,     // Stop TTS
        stop,             // Stop everything
        speak,            // Manually speak text
        clearSOS,         // Clear SOS state
    };
}

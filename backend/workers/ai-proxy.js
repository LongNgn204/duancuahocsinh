// backend/workers/ai-proxy.js
// Ch√∫ th√≠ch: Cloudflare Worker proxy - Workers AI @cf/openai/gpt-oss-120b
// H·ªó tr·ª£: SSE streaming, structured JSON output, 2-pass accuracy check,
// SOS 3 t·∫ßng (rules-first), memory compression, observability light tier

import { classifyRiskRules, getRedTierResponse } from './risk.js';
import { sanitizeInput } from './sanitize.js';
import { formatMessagesForLLM, getRecentMessages, createMemorySummary } from './memory.js';

// ============================================================================
// SYSTEM INSTRUCTIONS - Mentor t√¢m l√Ω h·ªçc ƒë∆∞·ªùng
// ============================================================================
const SYSTEM_INSTRUCTIONS = `B·∫°n l√† "B·∫°n ƒê·ªìng H√†nh" - m·ªôt mentor t√¢m l√Ω ·∫•m √°p, t√¥n tr·ªçng, kh√¥ng ph√°n x√©t cho h·ªçc sinh Vi·ªát Nam.

PHONG C√ÅCH
- Gi·ªçng vƒÉn th·∫•u c·∫£m, n√≥i ng·∫Øn g·ªçn (50-90 t·ª´), d√πng t·ª´ g·∫ßn g≈©i h·ªçc sinh
- Tr√°nh khu√¥n m·∫´u "T√¥i l√† AI", kh√¥ng robot
- Lu√¥n x√°c th·ª±c c·∫£m x√∫c tr∆∞·ªõc khi g·ª£i √Ω
- K·∫øt th√∫c b·∫±ng c√¢u h·ªèi m·ªü ph√π h·ª£p

QUY TR√åNH SUY LU·∫¨N (N·ªòI B·ªò - KH√îNG TI·∫æT L·ªò)
1. Nghe v√† nh·∫≠n di·ªán c·∫£m x√∫c ng∆∞·ªùi d√πng
2. Ph√¢n t√≠ch nguy√™n nh√¢n g·ªëc r·ªÖ
3. Tham chi·∫øu ki·∫øn th·ª©c t√¢m l√Ω h·ªçc ph√π h·ª£p
4. ƒê∆∞a ra g·ª£i √Ω an to√†n + c√¢u h·ªèi m·ªü

AN TO√ÄN
- Tr√°nh b·ªãa ƒë·∫∑t. N·∫øu thi·∫øu th√¥ng tin, n√≥i r√µ v√† h∆∞·ªõng h·ªçc sinh t·ªõi ng∆∞·ªùi l·ªõn/ngu·ªìn tin c·∫≠y
- Kh√¥ng ch·∫©n ƒëo√°n b·ªánh, kh√¥ng k√™ thu·ªëc
- Red flags: t·ª± h·∫°i, b·∫°o l·ª±c/l·∫°m d·ª•ng, tr·∫ßm c·∫£m k√©o d√†i ‚Üí h∆∞·ªõng t·ªõi hotline

OUTPUT FORMAT (B·∫ÆT BU·ªòC JSON)
{
  "riskLevel": "green|yellow|red",
  "emotion": "c·∫£m x√∫c nh·∫≠n di·ªán ƒë∆∞·ª£c",
  "reply": "c√¢u tr·∫£ l·ªùi th·∫•u c·∫£m 50-90 t·ª´",
  "nextQuestion": "c√¢u h·ªèi m·ªü ti·∫øp theo",
  "actions": ["g·ª£i √Ω 1", "g·ª£i √Ω 2"],
  "confidence": 0.0-1.0,
  "disclaimer": "disclaimer n·∫øu c·∫ßn ho·∫∑c null"
}`;

// ============================================================================
// CORS HELPERS
// ============================================================================
function getAllowedOrigin(request, env) {
  const reqOrigin = request.headers.get('Origin') || '';
  const allow = env.ALLOW_ORIGIN || '*';

  if (allow === '*' || !reqOrigin) return allow === '*' ? '*' : reqOrigin || '*';

  const list = allow.split(',').map((s) => s.trim());

  // Check exact match
  if (list.includes(reqOrigin)) return reqOrigin;

  // Check wildcard patterns (*.domain.com)
  for (const pattern of list) {
    if (pattern.startsWith('*.')) {
      const domain = pattern.slice(2);
      if (reqOrigin.endsWith('.' + domain) || reqOrigin.endsWith('//' + domain)) {
        return reqOrigin;
      }
      const originHost = reqOrigin.replace(/^https?:\/\//, '');
      if (originHost.endsWith('.' + domain) || originHost === domain) {
        return reqOrigin;
      }
    }
  }

  // Fallback: Cloudflare Pages preview URLs
  if (reqOrigin.includes('.pages.dev')) {
    return reqOrigin;
  }

  return 'null';
}

function corsHeaders(origin = '*') {
  return {
    'Access-Control-Allow-Origin': origin,
    'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
    'Access-Control-Allow-Headers': 'Content-Type, Authorization, x-requested-with',
    'Access-Control-Expose-Headers': 'X-Trace-Id',
  };
}

function json(data, status = 200, origin = '*', traceId) {
  return new Response(JSON.stringify(data), {
    status,
    headers: { 'Content-Type': 'application/json', ...corsHeaders(origin), ...(traceId ? { 'X-Trace-Id': traceId } : {}) },
  });
}

function handleOptions(request, env) {
  const origin = getAllowedOrigin(request, env);
  return new Response(null, { status: 204, headers: corsHeaders(origin) });
}

function sseHeaders(origin = '*', traceId) {
  return {
    ...corsHeaders(origin),
    'Content-Type': 'text/event-stream; charset=utf-8',
    'Cache-Control': 'no-cache',
    'X-Trace-Id': traceId
  };
}

// ============================================================================
// WORKERS AI CALLS
// ============================================================================

/**
 * G·ªçi Workers AI (non-stream)
 * @param {Object} env - Cloudflare env v·ªõi AI binding
 * @param {Array} messages - Messages array
 * @param {Object} options - Options
 * @returns {Promise<Object>} AI response
 */
async function callWorkersAI(env, messages, options = {}) {
  const model = options.model || env.MODEL || '@cf/meta/llama-3.1-8b-instruct';

  try {
    const result = await env.AI.run(model, {
      messages,
      temperature: options.temperature || 0.7,
      max_tokens: options.max_tokens || 512,
    });

    return result;
  } catch (error) {
    console.error('[WorkersAI] Error:', error.message);
    throw error;
  }
}

/**
 * G·ªçi Workers AI v·ªõi streaming
 * @param {Object} env - Cloudflare env v·ªõi AI binding
 * @param {Array} messages - Messages array
 * @param {Object} options - Options
 * @returns {ReadableStream} SSE stream
 */
async function callWorkersAIStream(env, messages, options = {}) {
  const model = options.model || env.MODEL || '@cf/meta/llama-3.1-8b-instruct';

  const result = await env.AI.run(model, {
    messages,
    temperature: options.temperature || 0.7,
    max_tokens: options.max_tokens || 512,
    stream: true,
  });

  return result;
}

/**
 * Parse JSON t·ª´ LLM response (c√≥ fallback)
 * @param {string} text - Raw response text
 * @returns {Object} Parsed JSON ho·∫∑c fallback object
 */
function parseAIResponse(text) {
  if (!text) {
    return createFallbackResponse('Kh√¥ng c√≥ ph·∫£n h·ªìi');
  }

  // T√¨m JSON trong response
  const jsonMatch = text.match(/\{[\s\S]*\}/);
  if (jsonMatch) {
    try {
      const parsed = JSON.parse(jsonMatch[0]);
      // Validate required fields
      if (parsed.reply && typeof parsed.reply === 'string') {
        return {
          riskLevel: parsed.riskLevel || 'green',
          emotion: parsed.emotion || '',
          reply: parsed.reply,
          nextQuestion: parsed.nextQuestion || '',
          actions: Array.isArray(parsed.actions) ? parsed.actions.slice(0, 4) : [],
          confidence: typeof parsed.confidence === 'number' ? parsed.confidence : 0.7,
          disclaimer: parsed.disclaimer || null,
        };
      }
    } catch (e) {
      console.error('[ParseAI] JSON parse error:', e.message);
    }
  }

  // Fallback: treat entire text as reply
  return createFallbackResponse(text);
}

function createFallbackResponse(text) {
  return {
    riskLevel: 'green',
    emotion: '',
    reply: text.slice(0, 500) || 'M√¨nh ƒëang l·∫Øng nghe b·∫°n. B·∫°n c√≥ th·ªÉ chia s·∫ª th√™m kh√¥ng?',
    nextQuestion: '',
    actions: [],
    confidence: 0.5,
    disclaimer: null,
  };
}

/**
 * 2-pass accuracy check khi confidence th·∫•p
 * @param {Object} env 
 * @param {Object} firstResponse 
 * @param {string} userMessage 
 * @returns {Promise<Object>} Verified response
 */
async function twoPassCheck(env, firstResponse, userMessage) {
  // N·∫øu confidence ƒë·ªß cao, kh√¥ng c·∫ßn pass 2
  if (firstResponse.confidence >= 0.6) {
    return firstResponse;
  }

  console.log('[2-Pass] Confidence th·∫•p, th·ª±c hi·ªán self-check...');

  const checkPrompt = `Ki·ªÉm tra l·∫°i c√¢u tr·∫£ l·ªùi sau v√† s·ª≠a n·∫øu c·∫ßn:

C√ÇU H·ªéI G·ªêC: ${userMessage}

C√ÇU TR·∫¢ L·ªúI DRAFT:
${JSON.stringify(firstResponse, null, 2)}

KI·ªÇM TRA:
1. C√≥ b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng?
2. C√≥ r·ªßi ro an to√†n kh√¥ng?
3. C√≥ ph√π h·ª£p v·ªõi SOS tier (${firstResponse.riskLevel}) kh√¥ng?
4. Gi·ªçng ƒëi·ªáu c√≥ th·∫•u c·∫£m kh√¥ng?

N·∫øu c·∫ßn s·ª≠a, tr·∫£ v·ªÅ JSON ho√†n ch·ªânh. N·∫øu kh√¥ng c·∫ßn s·ª≠a, tr·∫£ v·ªÅ JSON g·ªëc v·ªõi confidence cao h∆°n.`;

  try {
    const checkResult = await callWorkersAI(env, [
      { role: 'system', content: 'B·∫°n l√† chuy√™n gia ki·ªÉm tra ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi t√¢m l√Ω. Tr·∫£ v·ªÅ JSON.' },
      { role: 'user', content: checkPrompt }
    ], { temperature: 0.3 });

    const verified = parseAIResponse(checkResult.response || '');
    // Ensure confidence is updated
    if (verified.confidence < 0.6) verified.confidence = 0.65;
    return verified;
  } catch (e) {
    console.error('[2-Pass] Error:', e.message);
    // Fallback to first response
    firstResponse.confidence = 0.55;
    return firstResponse;
  }
}

// ============================================================================
// MAIN HANDLER
// ============================================================================
export default {
  async fetch(request, env) {
    const startTime = Date.now();
    const traceId = (crypto && crypto.randomUUID) ? crypto.randomUUID() : Math.random().toString(36).slice(2);
    const origin = getAllowedOrigin(request, env);

    // CORS preflight
    if (request.method === 'OPTIONS') return handleOptions(request, env);

    // Only POST allowed
    if (request.method !== 'POST') {
      return json({ error: 'method_not_allowed' }, 405, origin, traceId);
    }

    // Parse body
    let body;
    try {
      body = await request.json();
    } catch (_) {
      return json({ error: 'invalid_json' }, 400, origin, traceId);
    }

    const { message, history = [], memorySummary = '' } = body || {};

    // Validate message
    if (!message || typeof message !== 'string') {
      return json({ error: 'missing_message' }, 400, origin, traceId);
    }

    // Sanitize input
    let sanitizedMessage;
    try {
      sanitizedMessage = sanitizeInput(message);
    } catch (e) {
      console.log(`[Sanitize] Blocked: ${e.message}`, { traceId });
      return json({ error: 'invalid_input', reason: e.message }, 400, origin, traceId);
    }

    // ========================================================================
    // SOS CLASSIFICATION (RULES-FIRST)
    // ========================================================================
    const riskLevel = classifyRiskRules(sanitizedMessage, history);

    // Log observability (kh√¥ng log raw message)
    console.log(JSON.stringify({
      type: 'request',
      traceId,
      riskLevel,
      model: env.MODEL || '@cf/openai/gpt-oss-120b',
      historyCount: history.length,
    }));

    // RED tier: tr·∫£ response chu·∫©n, kh√¥ng g·ªçi LLM
    if (riskLevel === 'red') {
      const redResponse = getRedTierResponse();
      console.log(JSON.stringify({
        type: 'response',
        traceId,
        riskLevel: 'red',
        latencyMs: Date.now() - startTime,
        status: 200,
      }));

      // Check if streaming requested - emit SSE format
      const url = new URL(request.url);
      const wantsStream = url.searchParams.get('stream') === 'true' ||
        request.headers.get('Accept')?.includes('text/event-stream');

      if (wantsStream) {
        // Emit RED tier response as SSE stream
        const stream = new ReadableStream({
          start(controller) {
            const enc = new TextEncoder();
            const send = (line) => controller.enqueue(enc.encode(line));

            // Meta event
            send(`event: meta\n`);
            send(`data: ${JSON.stringify({ trace_id: traceId, riskLevel: 'red', sos: true })}\n\n`);

            // Send the reply text
            const replyText = redResponse.reply + '\n\nüìû ' + redResponse.actions.join('\nüìû ');
            send(`data: ${JSON.stringify({ type: 'delta', text: replyText })}\n\n`);

            // Done event
            send(`data: ${JSON.stringify({ type: 'done' })}\n\n`);

            controller.close();
          },
        });
        return new Response(stream, { status: 200, headers: sseHeaders(origin, traceId) });
      }

      return json(redResponse, 200, origin, traceId);
    }


    // ========================================================================
    // PREPARE MESSAGES FOR LLM
    // ========================================================================
    const messages = formatMessagesForLLM(
      SYSTEM_INSTRUCTIONS,
      getRecentMessages(history, 8),
      sanitizedMessage,
      memorySummary
    );

    // ========================================================================
    // CHECK IF STREAMING REQUESTED
    // ========================================================================
    const url = new URL(request.url);
    const wantsStream = url.searchParams.get('stream') === 'true' ||
      request.headers.get('Accept')?.includes('text/event-stream');

    try {
      if (wantsStream) {
        // ====================================================================
        // STREAMING RESPONSE
        // ====================================================================
        const aiStream = await callWorkersAIStream(env, messages);

        const stream = new ReadableStream({
          async start(controller) {
            const enc = new TextEncoder();
            const send = (line) => controller.enqueue(enc.encode(line));

            // Send meta event
            send(`event: meta\n`);
            send(`data: ${JSON.stringify({ trace_id: traceId, riskLevel })}\n\n`);

            try {
              let fullText = '';
              const reader = aiStream.getReader();
              let buffer = '';

              while (true) {
                const { value, done } = await reader.read();
                if (done) break;

                // Workers AI stream returns chunks as SSE-like format
                const chunk = typeof value === 'string' ? value : new TextDecoder().decode(value);
                buffer += chunk;

                // Parse SSE lines from buffer
                let lineEnd;
                while ((lineEnd = buffer.indexOf('\n')) !== -1) {
                  const line = buffer.slice(0, lineEnd).trim();
                  buffer = buffer.slice(lineEnd + 1);

                  if (line.startsWith('data: ')) {
                    const dataStr = line.slice(6);
                    if (dataStr === '[DONE]') {
                      continue;
                    }
                    try {
                      const parsed = JSON.parse(dataStr);
                      // Workers AI tr·∫£ v·ªÅ {"response":"text"} ho·∫∑c {"response":null} khi done
                      if (parsed.response && typeof parsed.response === 'string') {
                        fullText += parsed.response;
                        // Send delta event v·ªõi format chu·∫©n cho frontend
                        send(`data: ${JSON.stringify({ type: 'delta', text: parsed.response })}\n\n`);
                      }
                    } catch (_) {
                      // Skip non-JSON lines
                    }
                  }
                }
              }

              // Send done event
              send(`data: ${JSON.stringify({ type: 'done' })}\n\n`);

              // Log completion
              console.log(JSON.stringify({
                type: 'response',
                traceId,
                riskLevel,
                latencyMs: Date.now() - startTime,
                status: 200,

                stream: true,
              }));

            } catch (err) {
              const errPayload = { type: 'error', error: 'model_error', note: String(err?.message || err) };
              send(`data: ${JSON.stringify(errPayload)}\n\n`);
            } finally {
              controller.close();
            }
          },
        });

        return new Response(stream, { status: 200, headers: sseHeaders(origin, traceId) });

      } else {
        // ====================================================================
        // NON-STREAMING RESPONSE (v·ªõi 2-pass check)
        // ====================================================================
        const result = await callWorkersAI(env, messages);
        const rawResponse = result.response || '';

        // Parse response
        let parsed = parseAIResponse(rawResponse);

        // Override riskLevel t·ª´ rules n·∫øu kh√°c
        if (riskLevel === 'yellow' && parsed.riskLevel === 'green') {
          parsed.riskLevel = 'yellow';
        }

        // 2-pass accuracy check
        parsed = await twoPassCheck(env, parsed, sanitizedMessage);

        // Log completion
        console.log(JSON.stringify({
          type: 'response',
          traceId,
          riskLevel: parsed.riskLevel,
          confidence: parsed.confidence,
          latencyMs: Date.now() - startTime,
          status: 200,
          stream: false,
        }));

        return json(parsed, 200, origin, traceId);
      }

    } catch (e) {
      console.error(JSON.stringify({
        type: 'error',
        traceId,
        error: e.message,
        latencyMs: Date.now() - startTime,
      }));
      return json({ error: 'model_error', note: String(e?.message || e) }, 502, origin, traceId);
    }
  },
};

// backend/workers/ai-proxy.js
// Ch√∫ th√≠ch: Cloudflare Workers AI Proxy - S·ª≠ d·ª•ng @cf/meta/llama-3.1-8b-instruct
// H·ªó tr·ª£: SSE streaming, structured JSON output, 2-pass accuracy check,
// SOS 3 t·∫ßng (rules-first), memory compression, observability light tier

import { classifyRiskRules, getRedTierResponse } from './risk.js';
import { sanitizeInput } from './sanitize.js';
import { formatMessagesForLLM, getRecentMessages, createMemorySummary } from './memory.js';
import { checkTokenLimit, addTokenUsage, estimateTokens, countTokensAccurate } from './token-tracker.js';
import { createTraceContext, logModelCall, addTraceHeader } from './observability.js';
import { loadUserMemory, updateUserMemory, formatMemoryContext, incrementConversationCount } from './user-memory.js';

// ============================================================================
// SYSTEM INSTRUCTIONS - Mentor t√¢m l√Ω h·ªçc ƒë∆∞·ªùng v6.0 (ENHANCED INTELLIGENCE)
// ============================================================================
const PROMPT_VERSION = 'mentor-v6.0.0'; // Major upgrade: smarter brain, better context retention, natural flow

const SYSTEM_INSTRUCTIONS = `B·∫°n l√† "B·∫°n ƒê·ªìng H√†nh" - m·ªôt tr·ª£ l√Ω t√¢m l√Ω TR√ç TU·ªÜ NH√ÇN T·∫†O TH√îNG MINH d√†nh cho h·ªçc sinh Vi·ªát Nam (12-18 tu·ªïi). B·∫°n ƒë∆∞·ª£c trang b·ªã:
- üß† Kh·∫£ nƒÉng ph√¢n t√≠ch t√¢m l√Ω s√¢u s·∫Øc
- üí≠ Tr√≠ nh·ªõ ng·ªØ c·∫£nh d√†i h·∫°n (nh·ªõ to√†n b·ªô cu·ªôc tr√≤ chuy·ªán)
- üéØ K·ªπ nƒÉng ƒë·∫∑t c√¢u h·ªèi Socratic ƒë·ªÉ gi√∫p user t·ª± kh√°m ph√°
- ‚ù§Ô∏è Empathy ·ªü c·∫•p ƒë·ªô chuy√™n gia

üåü VAI TR√í C·ªêT L√ïI:
- Mentor t√¢m l√Ω TH√îNG MINH, nh·∫°y b√©n, kh√¥ng ch·ªâ l·∫Øng nghe m√† c√≤n PH√ÇN T√çCH s√¢u
- X∆∞ng "m√¨nh/b·∫°n" t·ª± nhi√™n, nh·∫•t qu√°n
- GI·ªÆ RANH GI·ªöI: ng∆∞·ªùi h·ªó tr·ª£ t√¢m l√Ω chuy√™n nghi·ªáp, KH√îNG ph·∫£i b·∫°n th√¢n/ng∆∞·ªùi y√™u
- M·ªói response PH·∫¢I unique, s√°ng t·∫°o, ph√π h·ª£p context
- LU√îN ph·∫£n h·ªìi b·∫±ng m·ªôt ƒëo·∫°n vƒÉn li·ªÅn m·∫°ch 2-5 c√¢u, t·ª± nhi√™n nh∆∞ n√≥i chuy·ªán face-to-face

üìõ TUY·ªÜT ƒê·ªêI KH√îNG:
- D√πng gi·ªçng c·ª£t nh·∫£, t√°n t·ªânh, ƒë√πa gi·ª°n thi·∫øu chuy√™n nghi·ªáp
- N√≥i "haha", "xinh y√™u", "d·ªÖ th∆∞∆°ng", "cute" - vi ph·∫°m ranh gi·ªõi
- ƒê∆∞a l·ªùi khuy√™n generic khi ch∆∞a hi·ªÉu r√µ t√¨nh hu·ªëng
- Ph√°n x√©t, d·∫°y ƒë·ªùi, ho·∫∑c t·ªè ra bi·∫øt tu·ªët
- H·ªèi l·∫°i nh·ªØng g√¨ ƒë√£ bi·∫øt t·ª´ context (t·ªëi k·ªµ!)
- N√≥i c√¢u chung chung v√¥ nghƒ©a nh∆∞ "C√≥ chuy·ªán g√¨ v·∫≠y?" khi h·ªç ƒë√£ n√≥i r√µ
- Response d√†i d√≤ng, lan man, m·∫•t tr·ªçng t√¢m

üéì 7 NGUY√äN T·∫ÆC TH√îNG MINH:
1. **CONTEXT IS KING** - S·ª≠ d·ª•ng t·ªëi ƒëa th√¥ng tin ƒë√£ bi·∫øt, tr√°nh h·ªèi l·∫°i
2. **ACKNOWLEDGE FIRST** - Lu√¥n th·ª´a nh·∫≠n c·∫£m x√∫c tr∆∞·ªõc khi l√†m g√¨ kh√°c
3. **ASK SMART QUESTIONS** - H·ªèi m·ªü, s√¢u, gi√∫p user t·ª± kh√°m ph√° v·∫•n ƒë·ªÅ
4. **VALIDATE EMOTIONS** - C·∫£m x√∫c c·∫ßn ƒë∆∞·ª£c c√¥ng nh·∫≠n tr∆∞·ªõc gi·∫£i ph√°p
5. **REMEMBER EVERYTHING** - Nh·ªõ t√™n, c√¢u chuy·ªán, pattern c·∫£m x√∫c c·ªßa user
6. **PERSONALIZE DEEPLY** - ƒêi·ªÅu ch·ªânh tone v√† ƒë·ªô s√¢u theo t·ª´ng user
7. **GUIDE, NOT FIX** - C√πng user t√¨m gi·∫£i ph√°p, kh√¥ng √°p ƒë·∫∑t

üß† TH√îNG TIN ƒê√É BI·∫æT V·ªÄ USER (CRITICAL - ƒê·ªåC K·ª∏!):
[USER_MEMORY_CONTEXT]

üí° S·ª¨ D·ª§NG CONTEXT NH∆Ø TH∆Ø∆†NG HI·ªÜU TR√ç TU·ªÜ:
- G·ªçi t√™n user ngay l·∫≠p t·ª©c n·∫øu ƒë√£ bi·∫øt (kh√¥ng h·ªèi l·∫°i!)
- Reference back: "L·∫ßn tr∆∞·ªõc b·∫°n c√≥ n√≥i v·ªÅ [topic]..."
- Pattern recognition: "M√¨nh ƒë·ªÉ √Ω b·∫°n th∆∞·ªùng c·∫£m th·∫•y [emotion] khi [situation]..."
- Proactive care: "H√¥m tr∆∞·ªõc b·∫°n lo v·ªÅ [issue], gi·ªù th·∫ø n√†o r·ªìi?"
- ƒêi·ªÅu ch·ªânh ƒë·ªô s√¢u: User m·ªõi ‚Üí gentle, User quen ‚Üí deeper psycho-analysis

üí¨ CHI·∫æN L∆Ø·ª¢C PH·∫¢N H·ªíI TH√îNG MINH:

[Greeting - hi/hello/xin ch√†o]
‚Üí N·∫øu bi·∫øt t√™n: "Ch√†o [t√™n]! [Observation v·ªÅ th·ªùi gian/ng√†y] H√¥m nay b·∫°n th·∫ø n√†o?"
‚Üí N·∫øu ch∆∞a bi·∫øt: "Ch√†o b·∫°n! M√¨nh l√† B·∫°n ƒê·ªìng H√†nh, ·ªü ƒë√¢y ƒë·ªÉ l·∫Øng nghe b·∫°n. B·∫°n mu·ªën m√¨nh g·ªçi b·∫°n l√† g√¨?"
‚Üí N·∫øu ƒë√£ g·∫∑p tr∆∞·ªõc: "Ch√†o l·∫°i [t√™n]! Vui v√¨ g·∫∑p b·∫°n. [Topic tr∆∞·ªõc] gi·ªù ra sao r·ªìi?"

[Chia s·∫ª c·∫£m x√∫c ti√™u c·ª±c nh·∫π - "bu·ªìn/stress/m·ªát"]
‚Üí Acknowledge + Validate: "Nghe b·∫°n r·ªìi. [Emotion] l√† c·∫£m gi√°c kh√≥ ch·ªãu nh·ªâ."
‚Üí Open-ended smart question: "C√≥ ƒëi·ªÅu g√¨ ƒëang l√†m b·∫°n c·∫£m th·∫•y nh∆∞ v·∫≠y kh√¥ng?"
‚Üí KH√îNG v·ªôi ƒë∆∞a gi·∫£i ph√°p - ƒë·ªÉ h·ªç n√≥i th√™m!

[Chia s·∫ª v·∫•n ƒë·ªÅ c·ª• th·ªÉ ƒë√£ n√™u r√µ]
‚Üí VALIDATE TR∆Ø·ªöC: "Nghe qua ƒëi·ªÅu n√†y th·∫≠t s·ª± [intensifier] v·ªõi b·∫°n."
‚Üí REFLECT BACK: "M√¨nh hi·ªÉu - b·∫°n ƒëang c·∫£m th·∫•y [emotion] v√¨ [reason h·ªç n√™u], ƒë√∫ng kh√¥ng?"
‚Üí DIG DEEPER: "ƒêi·ªÅu n√†y ·∫£nh h∆∞·ªüng ƒë·∫øn b·∫°n nh∆∞ th·∫ø n√†o? / B·∫°n nghƒ© g√¨ v·ªÅ t√¨nh hu·ªëng n√†y?"
‚Üí TUY·ªÜT ƒê·ªêI KH√îNG h·ªèi "C√≥ chuy·ªán g√¨ v·∫≠y?" khi h·ªç ƒë√£ n√™u r√µ!

Example:
User: "M√¨nh v·ª´a c√£i nhau v·ªõi b·∫°n th√¢n"
‚úÖ "√îi, c√£i nhau v·ªõi b·∫°n th√¢n th√¨ t·ªïn th∆∞∆°ng l·∫Øm. M√¨nh hi·ªÉu b·∫°n ƒëang bu·ªìn v√† c√≥ th·ªÉ h∆°i b·ªëi r·ªëi v·ªÅ chuy·ªán n√†y. B·∫°n c·∫£m th·∫•y th·∫ø n√†o ngay l√∫c n√†y, v√† ƒëi·ªÅu g√¨ ƒë√£ x·∫£y ra v·∫≠y?"
‚ùå "C√≥ chuy·ªán g√¨ khi·∫øn b·∫°n bu·ªìn v·∫≠y?" (ƒê√É N√ìI R√ï R·ªíI!)
‚ùå "ƒê·ª´ng bu·ªìn, b·∫°n ·∫•y s·∫Ω hi·ªÉu m√†." (Advice qu√° s·ªõm)

üö® T√åNH HU·ªêNG GIA ƒê√åNH NGHI√äM TR·ªåNG (b·∫°o l·ª±c/ƒë√°nh ƒë·∫≠p/x√¢m h·∫°i):
‚Üí VALIDATE NGAY: "M√¨nh r·∫•t ti·∫øc khi nghe ƒëi·ªÅu n√†y. ƒêi·ªÅu ƒë√≥ kh√¥ng bao gi·ªù n√™n x·∫£y ra v·ªõi b·∫°n."
‚Üí AN TO√ÄN TR∆Ø·ªöC: "B·∫°n c√≥ ƒëau kh√¥ng? B·∫°n c√≥ ƒëang an to√†n ngay b√¢y gi·ªù kh√¥ng?"
‚Üí ASSESS FREQUENCY: "Chuy·ªán n√†y c√≥ x·∫£y ra th∆∞·ªùng xuy√™n kh√¥ng? C√≥ ai bi·∫øt v·ªÅ ƒëi·ªÅu n√†y ch∆∞a?"
‚Üí GENTLE RESOURCE: "C√≥ ng∆∞·ªùi l·ªõn n√†o m√† b·∫°n c·∫£m th·∫•y tin t∆∞·ªüng ƒë·ªÉ n√≥i chuy·ªán kh√¥ng? Th·∫ßy c√¥, b√°c sƒ©, ho·∫∑c h·ªç h√†ng?"
‚Üí KH√îNG: ph√°n x√©t cha m·∫π, advice ph√°p l√Ω, n√≥i "h·ªç c√≥ l√Ω do", n√≥i "ƒë√≥ l√† b√¨nh th∆∞·ªùng"

Example:
User: "M·∫π ƒë√°nh t√¥i ph·∫£i l√†m sao"
‚úÖ "M√¨nh r·∫•t ti·∫øc khi nghe ƒëi·ªÅu n√†y. Vi·ªác b·ªã ƒë√°nh l√†m b·∫°n ƒëau c·∫£ th·ªÉ x√°c l·∫´n tinh th·∫ßn, v√† m√¨nh hi·ªÉu b·∫°n ƒëang r·∫•t kh√≥ khƒÉn b√¢y gi·ªù. Tr∆∞·ªõc ti√™n, b·∫°n c√≥ ƒëau kh√¥ng v√† b·∫°n c√≥ ƒëang an to√†n ·ªü ƒë√¢u ƒë√≥ ngay l√∫c n√†y kh√¥ng? M√¨nh mu·ªën hi·ªÉu r√µ h∆°n - chuy·ªán n√†y x·∫£y ra th∆∞·ªùng xuy√™n kh√¥ng?"
‚ùå "Chuy·ªán g√¨ khi·∫øn b·∫°n bu·ªìn v·∫≠y?" (THI·∫æU EMPATHY)
‚ùå "C√≥ l·∫Ω m·∫π b·∫°n ƒëang stress" (PH√ÅN X√âT)

[H·ªèi c·ª• th·ªÉ/ki·∫øn th·ª©c/t∆∞ v·∫•n h·ªçc t·∫≠p]
‚Üí Tr·∫£ l·ªùi ch√≠nh x√°c, s√∫c t√≠ch, h·ªØu √≠ch
‚Üí N·∫øu kh√¥ng ch·∫Øc: "M√¨nh kh√¥ng ch·∫Øc 100%, nh∆∞ng theo hi·ªÉu bi·∫øt th√¨ [answer]. B·∫°n c√≥ mu·ªën m√¨nh t√¨m hi·ªÉu k·ªπ h∆°n kh√¥ng?"
‚Üí Lu√¥n li√™n h·ªá v·ªÅ kh√≠a c·∫°nh t√¢m l√Ω n·∫øu c√≥: "V·ªÅ m·∫∑t h·ªçc t·∫≠p th√¨ [answer], c√≤n v·ªÅ c·∫£m x√∫c, b·∫°n c√≥ √°p l·ª±c kh√¥ng?"

[Follow-up conversation/Topic l·∫∑p l·∫°i]
‚Üí MEMORY FLEX: "·ª™m, l·∫ßn tr∆∞·ªõc b·∫°n c√≥ n√≥i v·ªÅ [topic] v√† l√∫c ƒë√≥ b·∫°n c·∫£m th·∫•y [emotion]. Gi·ªù t√¨nh h√¨nh th·∫ø n√†o r·ªìi?"
‚Üí PROGRESS CHECK: "M√¨nh nh·ªõ b·∫°n ƒëang g·∫∑p kh√≥ khƒÉn v·ªõi [issue]. C√≥ ti·∫øn tri·ªÉn g√¨ ch∆∞a?"
‚Üí Cho th·∫•y b·∫°n th·ª±c s·ª± quan t√¢m v√† nh·ªõ!

üö® SOS - T√åNH HU·ªêNG NGUY HI·ªÇM (t·ª± h·∫°i/t·ª± t·ª≠/b·∫°o l·ª±c nghi√™m tr·ªçng/x√¢m h·∫°i):
- Nghi√™m t√∫c, b√¨nh tƒ©nh, KH√îNG ho·∫£ng lo·∫°n
- Kh√¥ng c·ªë "fix" hay thuy·∫øt ph·ª•c h·ªç ng·ª´ng nghƒ© v·ªÅ vi·ªác ƒë√≥
- VALIDATE: "M√¨nh nghe b·∫°n r·ªìi, v√† m√¨nh r·∫•t lo l·∫Øng cho b·∫°n."
- EMPATHIZE: "Nh·ªØng g√¨ b·∫°n ƒëang c·∫£m th·∫•y nghe r·∫•t n·∫∑ng n·ªÅ v√† ƒëau ƒë·ªõn. M√¨nh hi·ªÉu."
- RESOURCE: "B·∫°n kh√¥ng ƒë∆°n ƒë·ªôc trong ƒëi·ªÅu n√†y. C√≥ nh·ªØng ng∆∞·ªùi chuy√™n nghi·ªáp s·∫µn s√†ng gi√∫p b·∫°n ngay l√∫c n√†y. B·∫°n c√≥ th·ªÉ g·ªçi 1800 599 920 (mi·ªÖn ph√≠, 24/7) ho·∫∑c nh·∫Øn tin cho m√¨nh ti·∫øp, m√¨nh v·∫´n ·ªü ƒë√¢y."
- ASSESS SAFETY: "B·∫°n c√≥ ƒëang ·ªü m·ªôt n∆°i an to√†n kh√¥ng?"

‚ú® V√ç D·ª§ RESPONSE TH√îNG MINH:

User: "·ªßa t√™n m√¨nh l√† g√¨ nh·ªâ" (sau khi ƒë√£ n√≥i t√™n l√† "Minh")
‚úÖ "B·∫°n l√† Minh m√†! M√¨nh c√≤n nh·ªõ b·∫°n gi·ªõi thi·ªáu l·∫ßn tr∆∞·ªõc ƒë·∫•y. Sao gi·ªù b·∫°n h·ªèi v·∫≠y, c√≥ chuy·ªán g√¨ khi·∫øn b·∫°n b·ªëi r·ªëi kh√¥ng?"
‚ùå "B·∫°n t√™n g√¨ v·∫≠y?" (MEMORY FAIL)

User: "thi r·ªõt r·ªìi"
‚úÖ "√îi, thi kh√¥ng ƒë·∫°t th√¨ frustrating v√† th·∫•t v·ªçng l·∫Øm, ƒë·∫∑c bi·ªát n·∫øu b·∫°n ƒë√£ c·ªë g·∫Øng. B·∫°n ƒëang c·∫£m th·∫•y th·∫ø n√†o v·ªÅ k·∫øt qu·∫£ n√†y? V√† c√≥ ai trong gia ƒë√¨nh ƒë√£ bi·∫øt ch∆∞a?"
‚ùå "ƒê·ª´ng bu·ªìn, l·∫ßn sau c·ªë g·∫Øng n·ªØa." (GENERIC ADVICE)

User: "B·∫°n th√¢n block m√¨nh" (l·∫ßn 2 nh·∫Øc ƒë·∫øn ng∆∞·ªùi b·∫°n n√†y)
‚úÖ "√îi, [t√™n b·∫°n th√¢n n·∫øu bi·∫øt] block b·∫°n √†? M√¨nh nh·ªõ l·∫ßn tr∆∞·ªõc b·∫°n c√≥ n√≥i hai b·∫°n ƒëang c√≥ ch√∫t cƒÉng th·∫≥ng. Gi·ªù b·∫°n c·∫£m th·∫•y th·∫ø n√†o, v√† b·∫°n c√≥ bi·∫øt l√Ω do t·∫°i sao kh√¥ng?"
‚ùå "C√≥ chuy·ªán g√¨ v·ªõi b·∫°n ·∫•y v·∫≠y?" (KH√îNG NH·ªö CONTEXT)

üì¶ OUTPUT FORMAT (JSON - KH√îNG ti·∫øt l·ªô cho user):
QUAN TR·ªåNG: "reply" PH·∫¢I l√† M·ªòT ƒëo·∫°n vƒÉn li·ªÅn m·∫°ch 2-5 c√¢u, t·ª± nhi√™n, KH√îNG ng·∫Øt d√≤ng.
{
  "riskLevel": "green|yellow|red",
  "emotion": "c·∫£m x√∫c ch√≠nh detected",
  "reply": "ph·∫£n h·ªìi 2-5 c√¢u LI·ªÄN M·∫†CH, t·ª± nhi√™n nh∆∞ n√≥i chuy·ªán. KH√îNG xu·ªëng d√≤ng. Th·ªÉ hi·ªán intelligence qua: acknowledge + empathy + smart question/insight.",
  "actions": ["t·ªëi ƒëa 2 g·ª£i √Ω SMARTER n·∫øu ph√π h·ª£p"],
  "confidence": 0.0-1.0,
  "reasoning": "1-2 c√¢u gi·∫£i th√≠ch t·∫°i sao b·∫°n ph·∫£n h·ªìi nh∆∞ v·∫≠y (internal, kh√¥ng show user)",
  "memoryUpdate": {
    "shouldRemember": true,
    "displayName": "t√™n n·∫øu user gi·ªõi thi·ªáu",
    "newFacts": ["facts m·ªõi v·ªÅ user"],
    "emotionPattern": "pattern c·∫£m x√∫c detected",
    "currentStruggle": "v·∫•n ƒë·ªÅ ƒëang g·∫∑p",
    "positiveAspect": "ƒëi·ªÉm t√≠ch c·ª±c",
    "relationshipDynamics": "th√¥ng tin v·ªÅ m·ªëi quan h·ªá (b·∫°n b√®/gia ƒë√¨nh) n·∫øu c√≥",
    "copingStrategies": "c√°ch user ƒëang cope v·ªõi stress n·∫øu detect ƒë∆∞·ª£c"
  }
}`;

// ============================================================================
// CORS HELPERS
// ============================================================================
function getAllowedOrigin(request, env) {
  const reqOrigin = request.headers.get('Origin') || '';
  const allow = env.ALLOW_ORIGIN || '*';

  if (allow === '*' || !reqOrigin) return allow === '*' ? '*' : reqOrigin || '*';

  const list = allow.split(',').map((s) => s.trim());

  // Check exact match
  if (list.includes(reqOrigin)) return reqOrigin;

  // Check wildcard patterns (*.domain.com)
  for (const pattern of list) {
    if (pattern.startsWith('*.')) {
      const domain = pattern.slice(2);
      if (reqOrigin.endsWith('.' + domain) || reqOrigin.endsWith('//' + domain)) {
        return reqOrigin;
      }
      const originHost = reqOrigin.replace(/^https?:\/\//, '');
      if (originHost.endsWith('.' + domain) || originHost === domain) {
        return reqOrigin;
      }
    }
  }

  // Fallback: Cloudflare Pages preview URLs
  if (reqOrigin.includes('.pages.dev')) {
    return reqOrigin;
  }

  return 'null';
}

function corsHeaders(origin = '*') {
  return {
    'Access-Control-Allow-Origin': origin,
    'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
    'Access-Control-Allow-Headers': 'Content-Type, Authorization, x-requested-with',
    'Access-Control-Expose-Headers': 'X-Trace-Id',
  };
}

function json(data, status = 200, origin = '*', traceId) {
  return new Response(JSON.stringify(data), {
    status,
    headers: { 'Content-Type': 'application/json', ...corsHeaders(origin), ...(traceId ? { 'X-Trace-Id': traceId } : {}) },
  });
}

function handleOptions(request, env) {
  const origin = getAllowedOrigin(request, env);
  return new Response(null, { status: 204, headers: corsHeaders(origin) });
}

function sseHeaders(origin = '*', traceId) {
  return {
    ...corsHeaders(origin),
    'Content-Type': 'text/event-stream; charset=utf-8',
    'Cache-Control': 'no-cache',
    'X-Trace-Id': traceId || ''
  };
}

// ============================================================================
// WORKERS AI CALLS
// ============================================================================

/**
 * G·ªçi Workers AI (non-stream)
 * @param {Object} env - Cloudflare env v·ªõi AI binding
 * @param {Array} messages - Messages array
 * @param {Object} options - Options
 * @returns {Promise<Object>} AI response
 */
async function callWorkersAI(env, messages, options = {}) {
  const model = options.model || env.MODEL || '@cf/meta/llama-3.1-8b-instruct';

  try {
    const result = await env.AI.run(model, {
      messages,
      temperature: options.temperature || 0.7,
      max_tokens: options.max_tokens || 512,
    });

    return result;
  } catch (error) {
    console.error('[WorkersAI] Error:', error.message);
    throw error;
  }
}

/**
 * G·ªçi Workers AI v·ªõi streaming
 * @param {Object} env - Cloudflare env v·ªõi AI binding
 * @param {Array} messages - Messages array
 * @param {Object} options - Options
 * @returns {ReadableStream} SSE stream
 */
async function callWorkersAIStream(env, messages, options = {}) {
  const model = options.model || env.MODEL || '@cf/meta/llama-3.1-8b-instruct';

  const result = await env.AI.run(model, {
    messages,
    temperature: options.temperature || 0.7,
    max_tokens: options.max_tokens || 512,
    stream: true,
  });

  return result;
}

/**
 * Parse JSON t·ª´ LLM response (c√≥ fallback)
 * @param {string} text - Raw response text
 * @returns {Object} Parsed JSON ho·∫∑c fallback object
 */
function parseAIResponse(text) {
  if (!text) {
    return createFallbackResponse('Kh√¥ng c√≥ ph·∫£n h·ªìi');
  }

  // T√¨m JSON trong response
  const jsonMatch = text.match(/\{[\s\S]*\}/);
  if (jsonMatch) {
    try {
      const parsed = JSON.parse(jsonMatch[0]);
      // Validate required fields
      if (parsed.reply && typeof parsed.reply === 'string') {
        return {
          riskLevel: parsed.riskLevel || 'green',
          emotion: parsed.emotion || '',
          reply: parsed.reply,
          nextQuestion: parsed.nextQuestion || '',
          actions: Array.isArray(parsed.actions) ? parsed.actions.slice(0, 4) : [],
          confidence: typeof parsed.confidence === 'number' ? parsed.confidence : 0.7,
          disclaimer: parsed.disclaimer || null,
        };
      }
    } catch (e) {
      console.error('[ParseAI] JSON parse error:', e.message);
    }
  }

  // Fallback: treat entire text as reply
  return createFallbackResponse(text);
}

function createFallbackResponse(text) {
  return {
    riskLevel: 'green',
    emotion: '',
    reply: text.slice(0, 500) || 'M√¨nh ƒëang l·∫Øng nghe b·∫°n. B·∫°n c√≥ th·ªÉ chia s·∫ª th√™m kh√¥ng?',
    nextQuestion: '',
    actions: [],
    confidence: 0.5,
    disclaimer: null,
  };
}

/**
 * 2-pass accuracy check khi confidence th·∫•p
 * @param {Object} env 
 * @param {Object} firstResponse 
 * @param {string} userMessage 
 * @returns {Promise<Object>} Verified response
 */
async function twoPassCheck(env, firstResponse, userMessage) {
  // N·∫øu confidence ƒë·ªß cao, kh√¥ng c·∫ßn pass 2
  if (firstResponse.confidence >= 0.6) {
    return firstResponse;
  }

  console.log('[2-Pass] Confidence th·∫•p, th·ª±c hi·ªán self-check...');

  const checkPrompt = `Ki·ªÉm tra l·∫°i c√¢u tr·∫£ l·ªùi sau v√† s·ª≠a n·∫øu c·∫ßn:

C√ÇU H·ªéI G·ªêC: ${userMessage}

C√ÇU TR·∫¢ L·ªúI DRAFT:
${JSON.stringify(firstResponse, null, 2)}

KI·ªÇM TRA:
1. C√≥ b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng?
2. C√≥ r·ªßi ro an to√†n kh√¥ng?
3. C√≥ ph√π h·ª£p v·ªõi SOS tier (${firstResponse.riskLevel}) kh√¥ng?
4. Gi·ªçng ƒëi·ªáu c√≥ th·∫•u c·∫£m kh√¥ng?

N·∫øu c·∫ßn s·ª≠a, tr·∫£ v·ªÅ JSON ho√†n ch·ªânh. N·∫øu kh√¥ng c·∫ßn s·ª≠a, tr·∫£ v·ªÅ JSON g·ªëc v·ªõi confidence cao h∆°n.`;

  try {
    const checkResult = await callWorkersAI(env, [
      { role: 'system', content: 'B·∫°n l√† chuy√™n gia ki·ªÉm tra ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi t√¢m l√Ω. Tr·∫£ v·ªÅ JSON.' },
      { role: 'user', content: checkPrompt }
    ], { temperature: 0.3 });

    const verified = parseAIResponse(checkResult.response || '');
    // Ensure confidence is updated
    if (verified.confidence < 0.6) verified.confidence = 0.65;
    return verified;
  } catch (e) {
    console.error('[2-Pass] Error:', e.message);
    // Fallback to first response
    firstResponse.confidence = 0.55;
    return firstResponse;
  }
}

// ============================================================================
// MAIN HANDLER
// ============================================================================
export default {
  async fetch(request, env) {
    // T·∫°o trace context cho observability
    const trace = createTraceContext(request, env);
    const startTime = Date.now();
    const origin = getAllowedOrigin(request, env);

    // CORS preflight
    if (request.method === 'OPTIONS') return handleOptions(request, env);

    // Only POST allowed
    if (request.method !== 'POST') {
      trace.logResponse(405);
      return addTraceHeader(json({ error: 'method_not_allowed' }, 405, origin), trace.traceId);
    }

    // Parse body
    let body;
    try {
      body = await request.json();
    } catch (_) {
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'invalid_json' }, 400, origin), trace.traceId);
    }

    const { message, history = [], memorySummary = '', userId = null, userName = null } = body || {};

    // Validate message
    if (!message || typeof message !== 'string') {
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'missing_message' }, 400, origin), trace.traceId);
    }

    // Sanitize input
    let sanitizedMessage;
    try {
      sanitizedMessage = sanitizeInput(message);
    } catch (e) {
      trace.log('warn', 'input_sanitized', { reason: e.message });
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'invalid_input', reason: e.message }, 400, origin), trace.traceId);
    }

    // ========================================================================
    // SOS CLASSIFICATION (RULES-FIRST) - Enhanced v·ªõi context-aware
    // ========================================================================
    const riskLevel = classifyRiskRules(sanitizedMessage, history);

    // Log request v·ªõi observability
    trace.log('info', 'chat_request', {
      risk_level: riskLevel,
      model: env.MODEL || '@cf/meta/llama-3.1-8b-instruct',
      history_count: history.length,
      has_memory_summary: !!memorySummary,
    });

    // Real-time monitoring: Log SOS events v·ªõi structured logging
    if (riskLevel === 'red' || riskLevel === 'yellow') {
      trace.log('warn', 'sos_detected', {
        risk_level: riskLevel,
        message_length: sanitizedMessage.length,
        history_count: history.length,
        // Kh√¥ng log raw message ƒë·ªÉ b·∫£o v·ªá privacy
      });

      // C√≥ th·ªÉ g·ª≠i alert ƒë·∫øn admin n·∫øu c·∫ßn (future enhancement)
      // await sendAdminAlert(env, { riskLevel, traceId: trace.traceId });
    }

    // RED tier: tr·∫£ response chu·∫©n, kh√¥ng g·ªçi LLM
    if (riskLevel === 'red') {
      const redResponse = getRedTierResponse();
      trace.logResponse(200, { risk_level: 'red', sos: true });

      // Check if streaming requested - emit SSE format
      const url = new URL(request.url);
      const wantsStream = url.searchParams.get('stream') === 'true' ||
        request.headers.get('Accept')?.includes('text/event-stream');

      if (wantsStream) {
        // Emit RED tier response as SSE stream
        const stream = new ReadableStream({
          start(controller) {
            const enc = new TextEncoder();
            const send = (line) => controller.enqueue(enc.encode(line));

            // Meta event
            send(`event: meta\n`);
            send(`data: ${JSON.stringify({ trace_id: trace.traceId, riskLevel: 'red', sos: true })}\n\n`);

            // Send the reply text
            const replyText = redResponse.reply + '\n\nüìû ' + redResponse.actions.join('\nüìû ');
            send(`data: ${JSON.stringify({ type: 'delta', text: replyText })}\n\n`);

            // Done event
            send(`data: ${JSON.stringify({ type: 'done' })}\n\n`);

            controller.close();
          },
        });
        return new Response(stream, { status: 200, headers: sseHeaders(origin, trace.traceId) });
      }

      return addTraceHeader(json(redResponse, 200, origin), trace.traceId);
    }


    // ========================================================================
    // CHECK TOKEN LIMIT
    // ========================================================================
    const tokenCheck = await checkTokenLimit(env);
    if (!tokenCheck.allowed) {
      trace.log('warn', 'token_limit_exceeded', {
        tokens: tokenCheck.tokens,
        limit: tokenCheck.limit,
      });
      trace.logResponse(429);
      return addTraceHeader(json({
        error: 'token_limit_exceeded',
        message: 'ƒê√£ ƒë·∫°t gi·ªõi h·∫°n s·ª≠ d·ª•ng th√°ng n√†y. Vui l√≤ng th·ª≠ l·∫°i v√†o th√°ng sau.',
        tokens: tokenCheck.tokens,
        limit: tokenCheck.limit,
      }, 429, origin), trace.traceId);
    }

    // ========================================================================
    // RAG: Retrieve relevant context t·ª´ knowledge base
    // ========================================================================
    let ragContext = '';
    let usedRAG = 0;
    try {
      // Query knowledge base t·ª´ D1
      const kbResult = await env.ban_dong_hanh_db.prepare(
        `SELECT id, title, content, category, tags FROM knowledge_base 
         WHERE content LIKE ? OR title LIKE ? OR tags LIKE ?
         LIMIT 20`
      ).bind(
        `%${sanitizedMessage.slice(0, 50)}%`, // Search trong content
        `%${sanitizedMessage.slice(0, 30)}%`, // Search trong title
        `%${sanitizedMessage.slice(0, 30)}%`  // Search trong tags
      ).all().catch(() => ({ results: [] }));

      const knowledgeBase = kbResult.results.map(doc => ({
        id: doc.id,
        content: `${doc.title}\n${doc.content}`,
        category: doc.category,
        source: 'knowledge_base',
        tags: doc.tags ? JSON.parse(doc.tags) : [],
        // Note: embedding s·∫Ω ƒë∆∞·ª£c load t·ª´ DB trong hybridSearch n·∫øu c√≥
      }));

      if (knowledgeBase.length > 0) {
        // Import RAG functions
        const { hybridSearch, formatRAGContext } = await import('./rag.js');

        const retrievedDocs = await hybridSearch(
          sanitizedMessage,
          knowledgeBase,
          env,
          { topK: 3, bm25Weight: 0.6, denseWeight: 0.4 }
        ).catch(async () => {
          // Fallback to BM25 only n·∫øu hybrid search fail
          const { bm25Search } = await import('./rag.js');
          return bm25Search(sanitizedMessage, knowledgeBase).slice(0, 3);
        });

        if (retrievedDocs && retrievedDocs.length > 0) {
          const { formatRAGContext } = await import('./rag.js');
          ragContext = formatRAGContext(retrievedDocs);
          usedRAG = 1;
          trace.log('info', 'rag_used', {
            docs_retrieved: retrievedDocs.length,
            categories: retrievedDocs.map(d => d.category).join(',')
          });
        }
      }
    } catch (error) {
      // RAG l√† optional, kh√¥ng block n·∫øu l·ªói
      trace.log('warn', 'rag_retrieval_failed', { error: error.message });
    }

    // ========================================================================
    // LOAD USER MEMORY (Persistent context cho t·ª´ng user)
    // ========================================================================
    let userMemory = null;
    let userMemoryContext = 'ƒê√¢y l√† l·∫ßn ƒë·∫ßu ti√™n g·∫∑p user n√†y.';

    if (userId) {
      try {
        userMemory = await loadUserMemory(env, userId);
        userMemoryContext = formatMemoryContext(userMemory);
        trace.log('info', 'user_memory_loaded', {
          user_id: userId,
          trust_level: userMemory?.trustLevel || 'new',
          total_conversations: userMemory?.totalConversations || 0
        });
      } catch (error) {
        trace.log('warn', 'user_memory_load_failed', { error: error.message });
        // Continue without memory - fallback to stateless
      }
    }

    // Explicitly add userName if provided from frontend
    if (userName) {
      userMemoryContext = `T√™n c·ªßa user l√†: ${userName}.\n` + userMemoryContext;
    }

    // ========================================================================
    // PREPARE MESSAGES FOR LLM (v·ªõi RAG context + User Memory)
    // ========================================================================
    // Inject user memory v√†o system prompt
    let systemPromptWithContext = SYSTEM_INSTRUCTIONS.replace(
      '[USER_MEMORY_CONTEXT]',
      userMemoryContext
    );

    // Th√™m RAG context v√†o system prompt n·∫øu c√≥
    if (ragContext) {
      systemPromptWithContext = systemPromptWithContext + ragContext;
    }

    const messages = formatMessagesForLLM(
      systemPromptWithContext,
      getRecentMessages(history, 8),
      sanitizedMessage,
      memorySummary
    );

    // Estimate tokens for this request (c·∫£i thi·ªán accuracy)
    const estimatedTokens = countTokensAccurate(
      JSON.stringify(messages) + sanitizedMessage,
      env.MODEL || '@cf/meta/llama-3.1-8b-instruct'
    );

    // ========================================================================
    // CHECK IF STREAMING REQUESTED
    // ========================================================================
    const url = new URL(request.url);
    const wantsStream = url.searchParams.get('stream') === 'true' ||
      request.headers.get('Accept')?.includes('text/event-stream');

    try {
      if (wantsStream) {
        // ====================================================================
        // STREAMING RESPONSE
        // ====================================================================
        const aiStream = await callWorkersAIStream(env, messages);

        const stream = new ReadableStream({
          async start(controller) {
            const enc = new TextEncoder();
            const send = (line) => controller.enqueue(enc.encode(line));

            // Send meta event
            send(`event: meta\n`);
            send(`data: ${JSON.stringify({ trace_id: trace.traceId, riskLevel })}\n\n`);

            try {
              let fullText = '';
              const reader = aiStream.getReader();
              let buffer = '';

              while (true) {
                const { value, done } = await reader.read();
                if (done) break;

                // Workers AI stream returns chunks as SSE-like format
                const chunk = typeof value === 'string' ? value : new TextDecoder().decode(value);
                buffer += chunk;

                // Parse SSE lines from buffer
                let lineEnd;
                while ((lineEnd = buffer.indexOf('\n')) !== -1) {
                  const line = buffer.slice(0, lineEnd).trim();
                  buffer = buffer.slice(lineEnd + 1);

                  if (line.startsWith('data: ')) {
                    const dataStr = line.slice(6);
                    if (dataStr === '[DONE]') {
                      continue;
                    }
                    try {
                      const parsed = JSON.parse(dataStr);
                      // Workers AI tr·∫£ v·ªÅ {"response":"text"} ho·∫∑c {"response":null} khi done
                      if (parsed.response && typeof parsed.response === 'string') {
                        fullText += parsed.response;
                        // Send delta event v·ªõi format chu·∫©n cho frontend
                        send(`data: ${JSON.stringify({ type: 'delta', text: parsed.response })}\n\n`);
                      }
                    } catch (_) {
                      // Skip non-JSON lines
                    }
                  }
                }
              }

              // Send done event
              send(`data: ${JSON.stringify({ type: 'done' })}\n\n`);

              // Update token usage (estimate t·ª´ full text)
              const responseTokens = estimateTokens(fullText);
              const totalTokens = estimatedTokens + responseTokens;
              const tokenUsageResult = await addTokenUsage(env, totalTokens);

              // T√≠nh cost (∆∞·ªõc t√≠nh: $0.0005 per 1k tokens cho llama-3.1-8b)
              const costUsd = (totalTokens / 1000) * 0.0005;
              const model = env.MODEL || '@cf/meta/llama-3.1-8b-instruct';
              const modelVersion = model.split('@cf/')[1] || 'llama-3.1-8b-instruct';

              // Log model call v·ªõi tokens v√† cost
              const streamLatencyMs = Date.now() - startTime;
              trace.logModelCall(model, modelVersion, estimatedTokens, responseTokens, costUsd, streamLatencyMs);

              // Log completion
              trace.logResponse(200, {
                risk_level: riskLevel,
                tokens_in: estimatedTokens,
                tokens_out: responseTokens,
                tokens_total: totalTokens,
                cost_usd: costUsd,
                stream: true,
                token_usage: tokenUsageResult.tokens,
                token_warning: tokenUsageResult.warning,
              });

              // Log streaming response v√†o chat_responses (sau khi stream complete)
              // Note: V·ªõi streaming, ch√∫ng ta log sau khi c√≥ full response
              // ƒê·ªÉ ƒë∆°n gi·∫£n, log v·ªõi partial response v√† update sau n·∫øu c·∫ßn
              try {
                await env.ban_dong_hanh_db.prepare(
                  `INSERT INTO chat_responses 
                   (user_id, message_id, user_message, ai_response, risk_level, confidence, tokens_used, latency_ms, used_rag, prompt_version)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`
                ).bind(
                  userId || null,
                  trace.traceId,
                  sanitizedMessage.slice(0, 1000),
                  '[STREAMING]', // Placeholder, c√≥ th·ªÉ update sau
                  riskLevel || 'green',
                  0.8, // Default confidence
                  totalTokens,
                  streamLatencyMs,
                  usedRAG,
                  PROMPT_VERSION
                ).run().catch(err => {
                  trace.log('warn', 'stream_response_log_failed', { error: err.message });
                });
              } catch (err) {
                trace.log('warn', 'stream_response_log_error', { error: err.message });
              }

            } catch (err) {
              trace.logError(err, { stream: true });
              const errPayload = { type: 'error', error: 'model_error', note: String(err?.message || err) };
              send(`data: ${JSON.stringify(errPayload)}\n\n`);
            } finally {
              controller.close();
            }
          },
        });

        return new Response(stream, { status: 200, headers: sseHeaders(origin, trace.traceId) });

      } else {
        // ====================================================================
        // NON-STREAMING RESPONSE (v·ªõi 2-pass check)
        // ====================================================================
        const result = await callWorkersAI(env, messages);
        const rawResponse = result.response || '';

        // Parse response
        let parsed = parseAIResponse(rawResponse);

        // Override riskLevel t·ª´ rules n·∫øu kh√°c
        if (riskLevel === 'yellow' && parsed.riskLevel === 'green') {
          parsed.riskLevel = 'yellow';
        }

        // 2-pass accuracy check
        parsed = await twoPassCheck(env, parsed, sanitizedMessage);

        // Update token usage (estimate t·ª´ response)
        const responseTokens = estimateTokens(parsed.reply || '');
        const totalTokens = estimatedTokens + responseTokens;
        const tokenUsageResult = await addTokenUsage(env, totalTokens);

        // T√≠nh cost (∆∞·ªõc t√≠nh: $0.0005 per 1k tokens cho llama-3.1-8b)
        const costUsd = (totalTokens / 1000) * 0.0005;
        const model = env.MODEL || '@cf/meta/llama-3.1-8b-instruct';
        const modelVersion = model.split('@cf/')[1] || 'llama-3.1-8b-instruct';

        // Log model call v·ªõi tokens v√† cost
        trace.logModelCall(model, modelVersion, estimatedTokens, responseTokens, costUsd, Date.now() - startTime);

        // Log completion
        trace.logResponse(200, {
          risk_level: parsed.riskLevel,
          confidence: parsed.confidence,
          tokens_in: estimatedTokens,
          tokens_out: responseTokens,
          tokens_total: totalTokens,
          cost_usd: costUsd,
          stream: false,
          token_usage: tokenUsageResult.tokens,
          token_warning: tokenUsageResult.warning,
        });

        // ================================================================
        // UPDATE USER MEMORY (sau khi c√≥ response t·ª´ AI)
        // ================================================================
        if (userId && parsed.memoryUpdate) {
          try {
            await updateUserMemory(env, userId, parsed.memoryUpdate, sanitizedMessage, trace.traceId);
            trace.log('info', 'user_memory_updated', {
              user_id: userId,
              new_facts_count: parsed.memoryUpdate?.newFacts?.length || 0,
              emotion: parsed.memoryUpdate?.emotionPattern || null
            });
          } catch (error) {
            trace.log('warn', 'user_memory_update_failed', { error: error.message });
            // Non-blocking - continue to return response
          }
        }

        // Remove memoryUpdate from response (internal only)
        const { memoryUpdate, ...responseWithoutMemory } = parsed;

        return addTraceHeader(json(responseWithoutMemory, 200, origin), trace.traceId);
      }

    } catch (e) {
      trace.logError(e, { route: 'ai:chat' });
      trace.logResponse(502);
      return addTraceHeader(json({ error: 'model_error', note: String(e?.message || e) }, 502, origin), trace.traceId);
    }
  },
};

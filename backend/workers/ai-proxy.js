// backend/workers/ai-proxy.js
// Ch√∫ th√≠ch: Cloudflare Workers AI Proxy - S·ª≠ d·ª•ng @cf/meta/llama-3.1-8b-instruct
// H·ªó tr·ª£: SSE streaming, structured JSON output, 2-pass accuracy check,
// SOS 3 t·∫ßng (rules-first), memory compression, observability light tier

import { classifyRiskRules, getRedTierResponse } from './risk.js';
import { sanitizeInput } from './sanitize.js';
import { formatMessagesForLLM, getRecentMessages, createMemorySummary } from './memory.js';
import { checkTokenLimit, addTokenUsage, estimateTokens } from './token-tracker.js';
import { createTraceContext, logModelCall } from './observability.js';

// ============================================================================
// SYSTEM INSTRUCTIONS - Mentor t√¢m l√Ω h·ªçc ƒë∆∞·ªùng v2.1.0 (Phase 7 Enhanced)
// ============================================================================
const PROMPT_VERSION = 'mentor-v2.1.0'; // Semver cho versioning v√† A/B testing

const SYSTEM_INSTRUCTIONS = `B·∫°n l√† "B·∫°n ƒê·ªìng H√†nh" - m·ªôt mentor t√¢m l√Ω ·∫•m √°p, t√¥n tr·ªçng, kh√¥ng ph√°n x√©t cho h·ªçc sinh Vi·ªát Nam (12-18 tu·ªïi).

VAI TR√í: MENTOR T√ÇM L√ù (KH√îNG PH·∫¢I B√ÅC Sƒ®)
- B·∫°n l√† ng∆∞·ªùi b·∫°n ƒë·ªìng h√†nh, l·∫Øng nghe v√† h·ªó tr·ª£, KH√îNG ch·∫©n ƒëo√°n b·ªánh hay k√™ thu·ªëc
- M·ª•c ti√™u: gi√∫p h·ªçc sinh t·ª± kh√°m ph√° c·∫£m x√∫c, t√¨m gi·∫£i ph√°p t·ª´ b√™n trong
- T·∫°o kh√¥ng gian an to√†n ƒë·ªÉ h·ªçc sinh chia s·∫ª m√† kh√¥ng s·ª£ b·ªã ph√°n x√©t

PHONG C√ÅCH TR·∫¢ L·ªúI
- Th·∫•u c·∫£m tr∆∞·ªõc, g·ª£i √Ω sau: "M√¨nh hi·ªÉu c·∫£m gi√°c ƒë√≥ kh√≥ ch·ªãu l·∫Øm..."
- Ng·∫Øn g·ªçn (50-100 t·ª´), d√πng t·ª´ g·∫ßn g≈©i Gen Z t·ª± nhi√™n (kh√¥ng qu√° formal)
- TUY·ªÜT ƒê·ªêI KH√îNG n√≥i "T√¥i l√† AI", "l√† tr√≠ tu·ªá nh√¢n t·∫°o" - gi·ªØ gi·ªçng nh∆∞ ng∆∞·ªùi b·∫°n th√¢n
- X√°c th·ª±c c·∫£m x√∫c tr∆∞·ªõc khi ƒë∆∞a l·ªùi khuy√™n: "C·∫£m gi√°c ƒë√≥ ho√†n to√†n b√¨nh th∆∞·ªùng..."
- K·∫øt th√∫c b·∫±ng 1 c√¢u h·ªèi m·ªü gi√∫p h·ªçc sinh t·ª± suy ng·∫´m
- Tr√°nh robot, tr√°nh l·∫∑p l·∫°i c√¢u h·ªèi ƒë√£ h·ªèi trong context

PH∆Ø∆†NG PH√ÅP SOCRATIC (∆ØU TI√äN)
- Thay v√¨ ƒë∆∞a l·ªùi khuy√™n ngay, h·ªèi c√¢u h·ªèi gi√∫p t·ª± kh√°m ph√°:
  + "B·∫°n nghƒ© ƒëi·ªÅu g√¨ ƒëang l√†m b·∫°n c·∫£m th·∫•y nh∆∞ v·∫≠y?"
  + "N·∫øu b·∫°n th√¢n b·∫°n g·∫∑p t√¨nh hu·ªëng n√†y, b·∫°n s·∫Ω n√≥i g√¨ v·ªõi h·ªç?"
  + "C√≥ khi n√†o b·∫°n t·ª´ng v∆∞·ª£t qua c·∫£m gi√°c t∆∞∆°ng t·ª± kh√¥ng? L√∫c ƒë√≥ b·∫°n ƒë√£ l√†m g√¨?"
- Gi√∫p h·ªçc sinh t·ª± nh·∫≠n ra solution thay v√¨ √°p ƒë·∫∑t

PH√ÇN T√çCH NGUY√äN NH√ÇN G·ªêC
- Stress h·ªçc t·∫≠p: √°p l·ª±c ƒëi·ªÉm s·ªë, thi c·ª≠, so s√°nh v·ªõi b·∫°n b√®
- Gia ƒë√¨nh: m√¢u thu·∫´n b·ªë m·∫π, k·ª≥ v·ªçng cao, thi·∫øu th·∫•u hi·ªÉu
- B·∫°n b√®: b·ªã c√¥ l·∫≠p, xung ƒë·ªôt, ghosted, b·ªã b·∫Øt n·∫°t
- T√¨nh c·∫£m: th·∫•t t√¨nh, crush kh√¥ng ƒë√°p l·∫°i, b·ªã reject
- B·∫£n th√¢n: t·ª± ti, kh√¥ng bi·∫øt m√¨nh mu·ªën g√¨, identity crisis
- T∆∞∆°ng lai: lo l·∫Øng v·ªÅ ngh·ªÅ nghi·ªáp, kh√¥ng bi·∫øt ƒë∆∞·ªùng ƒëi

QUY TR√åNH SUY LU·∫¨N (N·ªòI B·ªò - KH√îNG TI·∫æT L·ªò)
1. Nh·∫≠n di·ªán c·∫£m x√∫c ch√≠nh (bu·ªìn/gi·∫≠n/s·ª£/lo l·∫Øng/stress/c√¥ ƒë∆°n/t·ªßi th√¢n/confused)
2. Ph·ªèng ƒëo√°n nguy√™n nh√¢n g·ªëc d·ª±a tr√™n danh s√°ch tr√™n
3. ƒê√°nh gi√° m·ª©c ƒë·ªô nghi√™m tr·ªçng (green/yellow/red)
4. N·∫øu green: L·∫Øng nghe + c√¢u h·ªèi Socratic + g·ª£i √Ω h√†nh ƒë·ªông nh·ªè
5. N·∫øu yellow: X√°c th·ª±c c·∫£m x√∫c s√¢u h∆°n + theo d√µi + ƒë·ªÅ xu·∫•t c·ª• th·ªÉ
6. N·∫øu red: Ph·∫£n h·ªìi an to√†n ngay l·∫≠p t·ª©c (xem ph·∫ßn AN TO√ÄN)

S·ª¨ D·ª§NG MEMORY/CONTEXT (QUAN TR·ªåNG)
- N·∫øu c√≥ context t·ª´ messages tr∆∞·ªõc, th·ªÉ hi·ªán s·ª± nh·ªõ m·ªôt c√°ch t·ª± nhi√™n:
  + "H√¥m tr∆∞·ªõc b·∫°n c√≥ chia s·∫ª v·ªÅ [ch·ªß ƒë·ªÅ]... M√¨nh th·∫•y b·∫°n ƒë√£ ti·∫øn b·ªô r·ªìi ƒë·∫•y!"
  + "M√¨nh nh·ªõ b·∫°n t·ª´ng n√≥i v·ªÅ [ƒëi·ªÅu g√¨ ƒë√≥]... B√¢y gi·ªù b·∫°n c·∫£m th·∫•y th·∫ø n√†o?"
- Theo d√µi s·ª± ti·∫øn b·ªô v√† c√¥ng nh·∫≠n: "M√¨nh th·∫•y b·∫°n ƒë√£ c·ªë g·∫Øng... Tuy·ªát v·ªùi!"
- Kh√¥ng l·∫∑p l·∫°i c√¢u h·ªèi ƒë√£ h·ªèi trong context g·∫ßn ƒë√¢y
- N·∫øu context c√≥ th√¥ng tin v·ªÅ nguy√™n nh√¢n g·ªëc (stress h·ªçc t·∫≠p, gia ƒë√¨nh, b·∫°n b√®), tham chi·∫øu l·∫°i m·ªôt c√°ch t·ª± nhi√™n
- S·ª≠ d·ª•ng memory summary ƒë·ªÉ hi·ªÉu b·ªëi c·∫£nh d√†i h·∫°n, kh√¥ng ch·ªâ tin nh·∫Øn g·∫ßn nh·∫•t

G·ª¢I √ù H√ÄNH ƒê·ªòNG (actions)
- Lu√¥n ƒë∆∞a 2-3 g·ª£i √Ω h√†nh ƒë·ªông c·ª• th·ªÉ, nh·ªè, d·ªÖ th·ª±c hi·ªán NGAY
- Link v·ªõi t√≠nh nƒÉng app: breathing, gratitude, focus, journal, games, sleep
- V√≠ d·ª•: "th·ª≠ b√†i th·ªü 4-7-8", "vi·∫øt 3 ƒëi·ªÅu bi·∫øt ∆°n", "focus 15 ph√∫t", "ch∆°i game th∆∞ gi√£n"
- G·ª£i √Ω d·ª±a tr√™n nguy√™n nh√¢n g·ªëc (stress h·ªçc t·∫≠p ‚Üí focus mode, c√¥ ƒë∆°n ‚Üí games/gratitude)

AN TO√ÄN (B·∫ÆT BU·ªòC - CHU·∫®N QUY·ªÄN L·ª¢I TR·∫∫ EM)
- KH√îNG b·ªãa ƒë·∫∑t s·ªë li·ªáu y khoa, ch·∫©n ƒëo√°n b·ªánh, k√™ thu·ªëc
- N·∫øu kh√¥ng ch·∫Øc: "M√¨nh kh√¥ng ch·∫Øc v·ªÅ ƒëi·ªÅu n√†y. B·∫°n n√™n h·ªèi th·∫ßy c√¥ ho·∫∑c ng∆∞·ªùi l·ªõn tin c·∫≠y nh√©!"
- RED FLAGS c·∫ßn can thi·ªáp ngay:
  + √ù ƒë·ªãnh t·ª± h·∫°i, t·ª± t·ª≠, mu·ªën ch·∫øt
  + D·∫•u hi·ªáu b·∫°o l·ª±c, l·∫°m d·ª•ng th·ªÉ ch·∫•t/t√¨nh d·ª•c
  + Tr·∫ßm c·∫£m/lo √¢u n·∫∑ng k√©o d√†i
  ‚Üí Ph·∫£n h·ªìi: "M√¨nh lo l·∫Øng cho b·∫°n. ƒê√¢y l√† t√¨nh hu·ªëng c·∫ßn s·ª± gi√∫p ƒë·ª° chuy√™n nghi·ªáp. H√£y li√™n h·ªá ngay: 1800 599 920 (mi·ªÖn ph√≠ 24/7) ho·∫∑c n√≥i v·ªõi ng∆∞·ªùi l·ªõn tin c·∫≠y nh√©. M√¨nh lu√¥n ·ªü ƒë√¢y c√πng b·∫°n."

OUTPUT FORMAT (B·∫ÆT BU·ªòC JSON)
{
  "riskLevel": "green|yellow|red",
  "emotion": "c·∫£m x√∫c ch√≠nh nh·∫≠n di·ªán (bu·ªìn/gi·∫≠n/s·ª£/lo/stress/c√¥ ƒë∆°n/t·ªßi th√¢n/confused)",
  "rootCause": "nguy√™n nh√¢n g·ªëc ph·ªèng ƒëo√°n (h·ªçc t·∫≠p/gia ƒë√¨nh/b·∫°n b√®/t√¨nh c·∫£m/b·∫£n th√¢n/t∆∞∆°ng lai)",
  "reply": "ph·∫£n h·ªìi th·∫•u c·∫£m 50-100 t·ª´ v·ªõi c√¢u h·ªèi Socratic",
  "nextQuestion": "c√¢u h·ªèi m·ªü gi√∫p t·ª± kh√°m ph√°",
  "actions": ["g·ª£i √Ω h√†nh ƒë·ªông 1", "g·ª£i √Ω 2", "g·ª£i √Ω 3 n·∫øu c·∫ßn"],
  "confidence": 0.0-1.0,
  "disclaimer": "disclaimer n·∫øu c·∫ßn ho·∫∑c null"
}`;

// ============================================================================
// CORS HELPERS
// ============================================================================
function getAllowedOrigin(request, env) {
  const reqOrigin = request.headers.get('Origin') || '';
  const allow = env.ALLOW_ORIGIN || '*';

  if (allow === '*' || !reqOrigin) return allow === '*' ? '*' : reqOrigin || '*';

  const list = allow.split(',').map((s) => s.trim());

  // Check exact match
  if (list.includes(reqOrigin)) return reqOrigin;

  // Check wildcard patterns (*.domain.com)
  for (const pattern of list) {
    if (pattern.startsWith('*.')) {
      const domain = pattern.slice(2);
      if (reqOrigin.endsWith('.' + domain) || reqOrigin.endsWith('//' + domain)) {
        return reqOrigin;
      }
      const originHost = reqOrigin.replace(/^https?:\/\//, '');
      if (originHost.endsWith('.' + domain) || originHost === domain) {
        return reqOrigin;
      }
    }
  }

  // Fallback: Cloudflare Pages preview URLs
  if (reqOrigin.includes('.pages.dev')) {
    return reqOrigin;
  }

  return 'null';
}

function corsHeaders(origin = '*') {
  return {
    'Access-Control-Allow-Origin': origin,
    'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
    'Access-Control-Allow-Headers': 'Content-Type, Authorization, x-requested-with',
    'Access-Control-Expose-Headers': 'X-Trace-Id',
  };
}

function json(data, status = 200, origin = '*', traceId) {
  return new Response(JSON.stringify(data), {
    status,
    headers: { 'Content-Type': 'application/json', ...corsHeaders(origin), ...(traceId ? { 'X-Trace-Id': traceId } : {}) },
  });
}

function handleOptions(request, env) {
  const origin = getAllowedOrigin(request, env);
  return new Response(null, { status: 204, headers: corsHeaders(origin) });
}

function sseHeaders(origin = '*', traceId) {
  return {
    ...corsHeaders(origin),
    'Content-Type': 'text/event-stream; charset=utf-8',
    'Cache-Control': 'no-cache',
    'X-Trace-Id': traceId || ''
  };
}

// ============================================================================
// WORKERS AI CALLS
// ============================================================================

/**
 * G·ªçi Workers AI (non-stream)
 * @param {Object} env - Cloudflare env v·ªõi AI binding
 * @param {Array} messages - Messages array
 * @param {Object} options - Options
 * @returns {Promise<Object>} AI response
 */
async function callWorkersAI(env, messages, options = {}) {
  const model = options.model || env.MODEL || '@cf/meta/llama-3.1-8b-instruct';

  try {
    const result = await env.AI.run(model, {
      messages,
      temperature: options.temperature || 0.7,
      max_tokens: options.max_tokens || 512,
    });

    return result;
  } catch (error) {
    console.error('[WorkersAI] Error:', error.message);
    throw error;
  }
}

/**
 * G·ªçi Workers AI v·ªõi streaming
 * @param {Object} env - Cloudflare env v·ªõi AI binding
 * @param {Array} messages - Messages array
 * @param {Object} options - Options
 * @returns {ReadableStream} SSE stream
 */
async function callWorkersAIStream(env, messages, options = {}) {
  const model = options.model || env.MODEL || '@cf/meta/llama-3.1-8b-instruct';

  const result = await env.AI.run(model, {
    messages,
    temperature: options.temperature || 0.7,
    max_tokens: options.max_tokens || 512,
    stream: true,
  });

  return result;
}

/**
 * Parse JSON t·ª´ LLM response (c√≥ fallback)
 * @param {string} text - Raw response text
 * @returns {Object} Parsed JSON ho·∫∑c fallback object
 */
function parseAIResponse(text) {
  if (!text) {
    return createFallbackResponse('Kh√¥ng c√≥ ph·∫£n h·ªìi');
  }

  // T√¨m JSON trong response
  const jsonMatch = text.match(/\{[\s\S]*\}/);
  if (jsonMatch) {
    try {
      const parsed = JSON.parse(jsonMatch[0]);
      // Validate required fields
      if (parsed.reply && typeof parsed.reply === 'string') {
        return {
          riskLevel: parsed.riskLevel || 'green',
          emotion: parsed.emotion || '',
          reply: parsed.reply,
          nextQuestion: parsed.nextQuestion || '',
          actions: Array.isArray(parsed.actions) ? parsed.actions.slice(0, 4) : [],
          confidence: typeof parsed.confidence === 'number' ? parsed.confidence : 0.7,
          disclaimer: parsed.disclaimer || null,
        };
      }
    } catch (e) {
      console.error('[ParseAI] JSON parse error:', e.message);
    }
  }

  // Fallback: treat entire text as reply
  return createFallbackResponse(text);
}

function createFallbackResponse(text) {
  return {
    riskLevel: 'green',
    emotion: '',
    reply: text.slice(0, 500) || 'M√¨nh ƒëang l·∫Øng nghe b·∫°n. B·∫°n c√≥ th·ªÉ chia s·∫ª th√™m kh√¥ng?',
    nextQuestion: '',
    actions: [],
    confidence: 0.5,
    disclaimer: null,
  };
}

/**
 * 2-pass accuracy check khi confidence th·∫•p
 * @param {Object} env 
 * @param {Object} firstResponse 
 * @param {string} userMessage 
 * @returns {Promise<Object>} Verified response
 */
async function twoPassCheck(env, firstResponse, userMessage) {
  // N·∫øu confidence ƒë·ªß cao, kh√¥ng c·∫ßn pass 2
  if (firstResponse.confidence >= 0.6) {
    return firstResponse;
  }

  console.log('[2-Pass] Confidence th·∫•p, th·ª±c hi·ªán self-check...');

  const checkPrompt = `Ki·ªÉm tra l·∫°i c√¢u tr·∫£ l·ªùi sau v√† s·ª≠a n·∫øu c·∫ßn:

C√ÇU H·ªéI G·ªêC: ${userMessage}

C√ÇU TR·∫¢ L·ªúI DRAFT:
${JSON.stringify(firstResponse, null, 2)}

KI·ªÇM TRA:
1. C√≥ b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng?
2. C√≥ r·ªßi ro an to√†n kh√¥ng?
3. C√≥ ph√π h·ª£p v·ªõi SOS tier (${firstResponse.riskLevel}) kh√¥ng?
4. Gi·ªçng ƒëi·ªáu c√≥ th·∫•u c·∫£m kh√¥ng?

N·∫øu c·∫ßn s·ª≠a, tr·∫£ v·ªÅ JSON ho√†n ch·ªânh. N·∫øu kh√¥ng c·∫ßn s·ª≠a, tr·∫£ v·ªÅ JSON g·ªëc v·ªõi confidence cao h∆°n.`;

  try {
    const checkResult = await callWorkersAI(env, [
      { role: 'system', content: 'B·∫°n l√† chuy√™n gia ki·ªÉm tra ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi t√¢m l√Ω. Tr·∫£ v·ªÅ JSON.' },
      { role: 'user', content: checkPrompt }
    ], { temperature: 0.3 });

    const verified = parseAIResponse(checkResult.response || '');
    // Ensure confidence is updated
    if (verified.confidence < 0.6) verified.confidence = 0.65;
    return verified;
  } catch (e) {
    console.error('[2-Pass] Error:', e.message);
    // Fallback to first response
    firstResponse.confidence = 0.55;
    return firstResponse;
  }
}

// ============================================================================
// MAIN HANDLER
// ============================================================================
export default {
  async fetch(request, env) {
    // T·∫°o trace context cho observability
    const trace = createTraceContext(request, env);
    const startTime = Date.now();
    const origin = getAllowedOrigin(request, env);

    // CORS preflight
    if (request.method === 'OPTIONS') return handleOptions(request, env);

    // Only POST allowed
    if (request.method !== 'POST') {
      trace.logResponse(405);
      return addTraceHeader(json({ error: 'method_not_allowed' }, 405, origin), trace.traceId);
    }

    // Parse body
    let body;
    try {
      body = await request.json();
    } catch (_) {
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'invalid_json' }, 400, origin), trace.traceId);
    }

    const { message, history = [], memorySummary = '' } = body || {};

    // Validate message
    if (!message || typeof message !== 'string') {
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'missing_message' }, 400, origin), trace.traceId);
    }

    // Sanitize input
    let sanitizedMessage;
    try {
      sanitizedMessage = sanitizeInput(message);
    } catch (e) {
      trace.log('warn', 'input_sanitized', { reason: e.message });
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'invalid_input', reason: e.message }, 400, origin), trace.traceId);
    }

    // ========================================================================
    // SOS CLASSIFICATION (RULES-FIRST) - Enhanced v·ªõi context-aware
    // ========================================================================
    const riskLevel = classifyRiskRules(sanitizedMessage, history);

    // Log request v·ªõi observability
    trace.log('info', 'chat_request', {
      risk_level: riskLevel,
      model: env.MODEL || '@cf/meta/llama-3.1-8b-instruct',
      history_count: history.length,
      has_memory_summary: !!memorySummary,
    });

    // Real-time monitoring: Log SOS events v·ªõi structured logging
    if (riskLevel === 'red' || riskLevel === 'yellow') {
      trace.log('warn', 'sos_detected', {
        risk_level: riskLevel,
        message_length: sanitizedMessage.length,
        history_count: history.length,
        // Kh√¥ng log raw message ƒë·ªÉ b·∫£o v·ªá privacy
      });
      
      // C√≥ th·ªÉ g·ª≠i alert ƒë·∫øn admin n·∫øu c·∫ßn (future enhancement)
      // await sendAdminAlert(env, { riskLevel, traceId: trace.traceId });
    }

    // RED tier: tr·∫£ response chu·∫©n, kh√¥ng g·ªçi LLM
    if (riskLevel === 'red') {
      const redResponse = getRedTierResponse();
      trace.logResponse(200, { risk_level: 'red', sos: true });

      // Check if streaming requested - emit SSE format
      const url = new URL(request.url);
      const wantsStream = url.searchParams.get('stream') === 'true' ||
        request.headers.get('Accept')?.includes('text/event-stream');

      if (wantsStream) {
        // Emit RED tier response as SSE stream
        const stream = new ReadableStream({
          start(controller) {
            const enc = new TextEncoder();
            const send = (line) => controller.enqueue(enc.encode(line));

            // Meta event
            send(`event: meta\n`);
            send(`data: ${JSON.stringify({ trace_id: trace.traceId, riskLevel: 'red', sos: true })}\n\n`);

            // Send the reply text
            const replyText = redResponse.reply + '\n\nüìû ' + redResponse.actions.join('\nüìû ');
            send(`data: ${JSON.stringify({ type: 'delta', text: replyText })}\n\n`);

            // Done event
            send(`data: ${JSON.stringify({ type: 'done' })}\n\n`);

            controller.close();
          },
        });
        return new Response(stream, { status: 200, headers: sseHeaders(origin, trace.traceId) });
      }

      return addTraceHeader(json(redResponse, 200, origin), trace.traceId);
    }


    // ========================================================================
    // CHECK TOKEN LIMIT
    // ========================================================================
    const tokenCheck = await checkTokenLimit(env);
    if (!tokenCheck.allowed) {
      trace.log('warn', 'token_limit_exceeded', {
        tokens: tokenCheck.tokens,
        limit: tokenCheck.limit,
      });
      trace.logResponse(429);
      return addTraceHeader(json({
        error: 'token_limit_exceeded',
        message: 'ƒê√£ ƒë·∫°t gi·ªõi h·∫°n s·ª≠ d·ª•ng th√°ng n√†y. Vui l√≤ng th·ª≠ l·∫°i v√†o th√°ng sau.',
        tokens: tokenCheck.tokens,
        limit: tokenCheck.limit,
      }, 429, origin), trace.traceId);
    }

    // ========================================================================
    // RAG: Retrieve relevant context t·ª´ knowledge base
    // ========================================================================
    let ragContext = '';
    let usedRAG = 0;
    try {
      // Query knowledge base t·ª´ D1
      const kbResult = await env.ban_dong_hanh_db.prepare(
        `SELECT id, title, content, category, tags FROM knowledge_base 
         WHERE content LIKE ? OR title LIKE ? OR tags LIKE ?
         LIMIT 20`
      ).bind(
        `%${sanitizedMessage.slice(0, 50)}%`, // Search trong content
        `%${sanitizedMessage.slice(0, 30)}%`, // Search trong title
        `%${sanitizedMessage.slice(0, 30)}%`  // Search trong tags
      ).all().catch(() => ({ results: [] }));

      const knowledgeBase = kbResult.results.map(doc => ({
        id: doc.id,
        content: `${doc.title}\n${doc.content}`,
        category: doc.category,
        source: 'knowledge_base',
        tags: doc.tags ? JSON.parse(doc.tags) : [],
        // Note: embedding s·∫Ω ƒë∆∞·ª£c load t·ª´ DB trong hybridSearch n·∫øu c√≥
      }));

      if (knowledgeBase.length > 0) {
        // Import RAG functions
        const { hybridSearch, formatRAGContext } = await import('./rag.js');
        
        const retrievedDocs = await hybridSearch(
          sanitizedMessage,
          knowledgeBase,
          env,
          { topK: 3, bm25Weight: 0.6, denseWeight: 0.4 }
        ).catch(async () => {
          // Fallback to BM25 only n·∫øu hybrid search fail
          const { bm25Search } = await import('./rag.js');
          return bm25Search(sanitizedMessage, knowledgeBase).slice(0, 3);
        });

        if (retrievedDocs && retrievedDocs.length > 0) {
          const { formatRAGContext } = await import('./rag.js');
          ragContext = formatRAGContext(retrievedDocs);
          usedRAG = 1;
          trace.log('info', 'rag_used', { 
            docs_retrieved: retrievedDocs.length,
            categories: retrievedDocs.map(d => d.category).join(',')
          });
        }
      }
    } catch (error) {
      // RAG l√† optional, kh√¥ng block n·∫øu l·ªói
      trace.log('warn', 'rag_retrieval_failed', { error: error.message });
    }

    // ========================================================================
    // PREPARE MESSAGES FOR LLM (v·ªõi RAG context)
    // ========================================================================
    // Th√™m RAG context v√†o system prompt n·∫øu c√≥
    const systemPromptWithRAG = ragContext 
      ? SYSTEM_INSTRUCTIONS + ragContext
      : SYSTEM_INSTRUCTIONS;
    
    const messages = formatMessagesForLLM(
      systemPromptWithRAG,
      getRecentMessages(history, 8),
      sanitizedMessage,
      memorySummary
    );

    // Estimate tokens for this request (c·∫£i thi·ªán accuracy)
    const estimatedTokens = countTokensAccurate(
      JSON.stringify(messages) + sanitizedMessage,
      env.MODEL || '@cf/meta/llama-3.1-8b-instruct'
    );

    // ========================================================================
    // CHECK IF STREAMING REQUESTED
    // ========================================================================
    const url = new URL(request.url);
    const wantsStream = url.searchParams.get('stream') === 'true' ||
      request.headers.get('Accept')?.includes('text/event-stream');

    try {
      if (wantsStream) {
        // ====================================================================
        // STREAMING RESPONSE
        // ====================================================================
        const aiStream = await callWorkersAIStream(env, messages);

        const stream = new ReadableStream({
          async start(controller) {
            const enc = new TextEncoder();
            const send = (line) => controller.enqueue(enc.encode(line));

            // Send meta event
            send(`event: meta\n`);
            send(`data: ${JSON.stringify({ trace_id: trace.traceId, riskLevel })}\n\n`);

            try {
              let fullText = '';
              const reader = aiStream.getReader();
              let buffer = '';

              while (true) {
                const { value, done } = await reader.read();
                if (done) break;

                // Workers AI stream returns chunks as SSE-like format
                const chunk = typeof value === 'string' ? value : new TextDecoder().decode(value);
                buffer += chunk;

                // Parse SSE lines from buffer
                let lineEnd;
                while ((lineEnd = buffer.indexOf('\n')) !== -1) {
                  const line = buffer.slice(0, lineEnd).trim();
                  buffer = buffer.slice(lineEnd + 1);

                  if (line.startsWith('data: ')) {
                    const dataStr = line.slice(6);
                    if (dataStr === '[DONE]') {
                      continue;
                    }
                    try {
                      const parsed = JSON.parse(dataStr);
                      // Workers AI tr·∫£ v·ªÅ {"response":"text"} ho·∫∑c {"response":null} khi done
                      if (parsed.response && typeof parsed.response === 'string') {
                        fullText += parsed.response;
                        // Send delta event v·ªõi format chu·∫©n cho frontend
                        send(`data: ${JSON.stringify({ type: 'delta', text: parsed.response })}\n\n`);
                      }
                    } catch (_) {
                      // Skip non-JSON lines
                    }
                  }
                }
              }

              // Send done event
              send(`data: ${JSON.stringify({ type: 'done' })}\n\n`);

              // Update token usage (estimate t·ª´ full text)
              const responseTokens = estimateTokens(fullText);
              const totalTokens = estimatedTokens + responseTokens;
              const tokenUsageResult = await addTokenUsage(env, totalTokens);
              
              // T√≠nh cost (∆∞·ªõc t√≠nh: $0.0005 per 1k tokens cho llama-3.1-8b)
              const costUsd = (totalTokens / 1000) * 0.0005;
              const model = env.MODEL || '@cf/meta/llama-3.1-8b-instruct';
              const modelVersion = model.split('@cf/')[1] || 'llama-3.1-8b-instruct';
              
              // Log model call v·ªõi tokens v√† cost
              const streamLatencyMs = Date.now() - startTime;
              trace.logModelCall(model, modelVersion, estimatedTokens, responseTokens, costUsd, streamLatencyMs);

              // Log completion
              trace.logResponse(200, {
                risk_level: riskLevel,
                tokens_in: estimatedTokens,
                tokens_out: responseTokens,
                tokens_total: totalTokens,
                cost_usd: costUsd,
                stream: true,
                token_usage: tokenUsageResult.tokens,
                token_warning: tokenUsageResult.warning,
              });

              // Log streaming response v√†o chat_responses (sau khi stream complete)
              // Note: V·ªõi streaming, ch√∫ng ta log sau khi c√≥ full response
              // ƒê·ªÉ ƒë∆°n gi·∫£n, log v·ªõi partial response v√† update sau n·∫øu c·∫ßn
              try {
                await env.ban_dong_hanh_db.prepare(
                  `INSERT INTO chat_responses 
                   (user_id, message_id, user_message, ai_response, risk_level, confidence, tokens_used, latency_ms, used_rag, prompt_version)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`
                ).bind(
                  userId || null,
                  trace.traceId,
                  sanitizedMessage.slice(0, 1000),
                  '[STREAMING]', // Placeholder, c√≥ th·ªÉ update sau
                  riskLevel || 'green',
                  0.8, // Default confidence
                  totalTokens,
                  streamLatencyMs,
                  usedRAG,
                  PROMPT_VERSION
                ).run().catch(err => {
                  trace.log('warn', 'stream_response_log_failed', { error: err.message });
                });
              } catch (err) {
                trace.log('warn', 'stream_response_log_error', { error: err.message });
              }

            } catch (err) {
              trace.logError(err, { stream: true });
              const errPayload = { type: 'error', error: 'model_error', note: String(err?.message || err) };
              send(`data: ${JSON.stringify(errPayload)}\n\n`);
            } finally {
              controller.close();
            }
          },
        });

        return new Response(stream, { status: 200, headers: sseHeaders(origin, trace.traceId) });

      } else {
        // ====================================================================
        // NON-STREAMING RESPONSE (v·ªõi 2-pass check)
        // ====================================================================
        const result = await callWorkersAI(env, messages);
        const rawResponse = result.response || '';

        // Parse response
        let parsed = parseAIResponse(rawResponse);

        // Override riskLevel t·ª´ rules n·∫øu kh√°c
        if (riskLevel === 'yellow' && parsed.riskLevel === 'green') {
          parsed.riskLevel = 'yellow';
        }

        // 2-pass accuracy check
        parsed = await twoPassCheck(env, parsed, sanitizedMessage);

        // Update token usage (estimate t·ª´ response)
        const responseTokens = estimateTokens(parsed.reply || '');
        const totalTokens = estimatedTokens + responseTokens;
        const tokenUsageResult = await addTokenUsage(env, totalTokens);
        
        // T√≠nh cost (∆∞·ªõc t√≠nh: $0.0005 per 1k tokens cho llama-3.1-8b)
        const costUsd = (totalTokens / 1000) * 0.0005;
        const model = env.MODEL || '@cf/meta/llama-3.1-8b-instruct';
        const modelVersion = model.split('@cf/')[1] || 'llama-3.1-8b-instruct';
        
        // Log model call v·ªõi tokens v√† cost
        trace.logModelCall(model, modelVersion, estimatedTokens, responseTokens, costUsd, Date.now() - startTime);

        // Log completion
        trace.logResponse(200, {
          risk_level: parsed.riskLevel,
          confidence: parsed.confidence,
          tokens_in: estimatedTokens,
          tokens_out: responseTokens,
          tokens_total: totalTokens,
          cost_usd: costUsd,
          stream: false,
          token_usage: tokenUsageResult.tokens,
          token_warning: tokenUsageResult.warning,
        });

        return addTraceHeader(json(parsed, 200, origin), trace.traceId);
      }

    } catch (e) {
      trace.logError(e, { route: 'ai:chat' });
      trace.logResponse(502);
      return addTraceHeader(json({ error: 'model_error', note: String(e?.message || e) }, 502, origin), trace.traceId);
    }
  },
};

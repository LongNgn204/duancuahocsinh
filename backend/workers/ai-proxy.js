// backend/workers/ai-proxy.js
// Ch√∫ th√≠ch: AI Chat module s·ª≠ d·ª•ng OpenAI ChatGPT (gpt-4o-mini - model r·∫ª nh·∫•t)
// Thay th·∫ø Vertex AI ƒë·ªÉ ti·∫øt ki·ªám chi ph√≠

import { classifyRiskRules, getRedTierResponse } from './risk.js';
import { sanitizeInput } from './sanitize.js';
import { formatMessagesForLLM, getRecentMessages, createMemorySummary } from './memory.js';
import { hybridSearch, formatRAGContext } from './rag.js';
import { redactPII } from './pii-redactor.js';

// ============================================================================
// CONFIGURATION
// ============================================================================

// Model r·∫ª nh·∫•t c·ªßa OpenAI: gpt-4o-mini (~$0.15/1M input, $0.60/1M output)
const OPENAI_MODEL = 'gpt-4o-mini';
const OPENAI_API_URL = 'https://api.openai.com/v1/chat/completions';

// Fallback: OpenRouter n·∫øu mu·ªën d√πng multi-provider
const OPENROUTER_API_URL = 'https://openrouter.ai/api/v1/chat/completions';
const OPENROUTER_MODEL = 'openai/gpt-4o-mini'; // Qua OpenRouter

// System prompt cho B·∫°n ƒê·ªìng H√†nh - ng∆∞·ªùi b·∫°n th·∫•u hi·ªÉu c·∫£m x√∫c h·ªçc sinh
const SYSTEM_PROMPT = `B·∫°n l√† "B·∫°n ƒê·ªìng H√†nh", m·ªôt ng∆∞·ªùi b·∫°n AI th√¢n thi·∫øt v√† th·∫•u hi·ªÉu c·∫£m x√∫c d√†nh cho h·ªçc sinh Vi·ªát Nam.

## B·∫†N L√Ä AI
- Ng∆∞·ªùi b·∫°n lu√¥n l·∫Øng nghe, th·∫•u hi·ªÉu v√† ƒë·ªìng c·∫£m
- Kh√¥ng ph√°n x√©t, kh√¥ng ch·ªâ tr√≠ch
- Ki√™n nh·∫´n, ·∫•m √°p v√† ƒë√°ng tin c·∫≠y
- Hi·ªÉu vƒÉn h√≥a v√† ng√¥n ng·ªØ Gen-Z Vi·ªát Nam

## B·∫†N C√ì TH·ªÇ GI√öP
- üí≠ T√¢m s·ª±: l·∫Øng nghe v√† chia s·∫ª khi bu·ªìn, stress, c√¥ ƒë∆°n
- üìö H·ªçc t·∫≠p: h·ªó tr·ª£ gi·∫£i ƒë√°p th·∫Øc m·∫Øc, ƒë·ªông vi√™n khi √°p l·ª±c
- üë®‚Äçüë©‚Äçüëß Gia ƒë√¨nh: hi·ªÉu v√† ƒë·ªìng c·∫£m v·ªõi m√¢u thu·∫´n gia ƒë√¨nh
- üíï B·∫°n b√®, t√¨nh c·∫£m: l·∫Øng nghe v√† chia s·∫ª kinh nghi·ªám
- üåü Ph√°t tri·ªÉn b·∫£n th√¢n: g·ª£i √Ω t√≠ch c·ª±c, x√¢y d·ª±ng t·ª± tin

## C√ÅCH N√ìI CHUY·ªÜN
1. N√≥i t·ª± nhi√™n nh∆∞ b·∫°n b√®, x∆∞ng "m√¨nh" - g·ªçi "b·∫°n" ho·∫∑c "c·∫≠u"
2. L·∫Øng nghe tr∆∞·ªõc, sau ƒë√≥ m·ªõi ƒë∆∞a l·ªùi khuy√™n (n·∫øu ƒë∆∞·ª£c h·ªèi)
3. Th·ªÉ hi·ªán s·ª± ƒë·ªìng c·∫£m: "M√¨nh hi·ªÉu c·∫£m gi√°c ƒë√≥...", "ƒêi·ªÅu ƒë√≥ ch·∫Øc kh√≥ khƒÉn l·∫Øm..."
4. Kh√¥ng gi·∫£ng ƒë·∫°o, kh√¥ng b·∫Øt bu·ªôc ph·∫£i l√†m g√¨
5. D√πng emoji nh·∫π nh√†ng: üíô üå∏ ‚ú® ü§ó üí™
6. N·∫øu kh√¥ng bi·∫øt ch·∫Øc, n√≥i th·∫≠t v√† g·ª£i √Ω t√¨m th√™m

## L∆ØU √ù QUAN TR·ªåNG
- N·∫øu b·∫°n c√≥ d·∫•u hi·ªáu kh·ªßng ho·∫£ng (t·ª± h·∫°i, mu·ªën ch·∫øt): NGAY L·∫¨P T·ª®C khuy√™n g·ªçi ƒë∆∞·ªùng d√¢y n√≥ng 111 v√† t√¨m ng∆∞·ªùi l·ªõn
- Kh√¥ng t∆∞ v·∫•n y t·∫ø, t√¢m l√Ω chuy√™n s√¢u - khuy√™n g·∫∑p chuy√™n gia n·∫øu c·∫ßn
- B·∫£o m·∫≠t: kh√¥ng h·ªèi th√¥ng tin c√° nh√¢n nh∆∞ ƒë·ªãa ch·ªâ, tr∆∞·ªùng, t√™n th·∫≠t

## ƒê·ªäNH D·∫†NG
- Markdown cho lists, bold khi c·∫ßn nh·∫•n m·∫°nh
- C√¢u ng·∫Øn g·ªçn, d·ªÖ ƒë·ªçc tr√™n ƒëi·ªán tho·∫°i
- LaTeX n·∫øu c√≥ c√¥ng th·ª©c: \\(...\\) inline, \\[...\\] block`;

// ============================================================================
// OPENAI API CALL
// ============================================================================

/**
 * G·ªçi OpenAI ChatGPT API
 * @param {Array} messages - Messages array [{role, content}]
 * @param {Object} env - Cloudflare env
 * @param {Object} options - {stream: boolean, maxTokens: number}
 * @returns {Promise<Response>} Response object
 */
async function callOpenAI(messages, env, options = {}) {
  const {
    stream = true,
    maxTokens = 1024,
    temperature = 0.7,
  } = options;

  // ∆Øu ti√™n OpenAI API key, fallback sang OpenRouter
  const apiKey = env.OPENAI_API_KEY || env.OPENROUTER_API_KEY;
  if (!apiKey) {
    throw new Error('Missing API key: Set OPENAI_API_KEY or OPENROUTER_API_KEY');
  }

  const useOpenRouter = !env.OPENAI_API_KEY && env.OPENROUTER_API_KEY;
  const apiUrl = useOpenRouter ? OPENROUTER_API_URL : OPENAI_API_URL;
  const model = useOpenRouter ? OPENROUTER_MODEL : OPENAI_MODEL;

  const body = {
    model,
    messages,
    max_tokens: maxTokens,
    temperature,
    stream,
  };

  // OpenRouter c·∫ßn th√™m headers
  const headers = {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${apiKey}`,
  };

  if (useOpenRouter) {
    headers['HTTP-Referer'] = 'https://duancuahocsinh.pages.dev';
    headers['X-Title'] = 'Ban Dong Hanh';
  }

  console.log(`[AI] Calling ${useOpenRouter ? 'OpenRouter' : 'OpenAI'} with model: ${model}`);

  const response = await fetch(apiUrl, {
    method: 'POST',
    headers,
    body: JSON.stringify(body),
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error('[AI] API Error:', response.status, errorText);
    throw new Error(`API Error: ${response.status} - ${errorText}`);
  }

  return response;
}

/**
 * Parse SSE stream t·ª´ OpenAI
 * @param {ReadableStream} stream 
 * @returns {AsyncGenerator<string>} Text chunks
 */
async function* parseSSEStream(stream) {
  const reader = stream.getReader();
  const decoder = new TextDecoder();
  let buffer = '';

  try {
    while (true) {
      const { done, value } = await reader.read();
      if (done) break;

      buffer += decoder.decode(value, { stream: true });
      const lines = buffer.split('\n');
      buffer = lines.pop() || ''; // Keep incomplete line

      for (const line of lines) {
        if (line.startsWith('data: ')) {
          const data = line.slice(6).trim();
          if (data === '[DONE]') return;

          try {
            const json = JSON.parse(data);
            const content = json.choices?.[0]?.delta?.content;
            if (content) yield content;
          } catch {
            // Ignore parse errors
          }
        }
      }
    }
  } finally {
    reader.releaseLock();
  }
}

// ============================================================================
// RAG - Retrieve context t·ª´ knowledge base
// ============================================================================

async function getRAGContext(env, query) {
  if (!env.ban_dong_hanh_db) return null;

  try {
    // L·∫•y t·∫•t c·∫£ documents t·ª´ knowledge_base
    const result = await env.ban_dong_hanh_db.prepare(
      'SELECT id, content, source, category FROM knowledge_base WHERE is_active = 1 LIMIT 100'
    ).all();

    if (!result.results || result.results.length === 0) {
      return null;
    }

    // Hybrid search
    const topDocs = await hybridSearch(query, result.results, env, {
      topK: 3,
      bm25Weight: 0.6,
      denseWeight: 0.4,
    });

    if (topDocs.length === 0) return null;

    return formatRAGContext(topDocs);
  } catch (error) {
    console.warn('[AI] RAG error:', error.message);
    return null;
  }
}

// ============================================================================
// MAIN HANDLER
// ============================================================================

export default {
  async fetch(request, env) {
    // Ch·ªâ accept POST
    if (request.method !== 'POST') {
      return new Response(JSON.stringify({ error: 'Method not allowed' }), {
        status: 405,
        headers: { 'Content-Type': 'application/json' },
      });
    }

    const t0 = Date.now();

    try {
      const body = await request.json();
      const { message, history = [], stream = true } = body;

      // Validate input
      let sanitizedMessage;
      try {
        sanitizedMessage = sanitizeInput(message);
      } catch (err) {
        return new Response(JSON.stringify({
          error: err.message,
          reply: 'Vui l√≤ng nh·∫≠p tin nh·∫Øn h·ª£p l·ªá.',
        }), {
          status: 400,
          headers: { 'Content-Type': 'application/json' },
        });
      }

      // PII Redaction
      const redactedMessage = redactPII(sanitizedMessage);

      // Risk classification
      const riskLevel = classifyRiskRules(redactedMessage, history);
      console.log('[AI] Risk level:', riskLevel);

      // RED tier - tr·∫£ v·ªÅ ngay v·ªõi hotline info
      if (riskLevel === 'red') {
        const redResponse = getRedTierResponse();
        return new Response(JSON.stringify(redResponse), {
          status: 200,
          headers: { 'Content-Type': 'application/json' },
        });
      }

      // Get RAG context (cho c√¢u h·ªèi h·ªçc thu·∫≠t)
      const ragContext = await getRAGContext(env, redactedMessage);

      // Create memory summary n·∫øu history d√†i
      const memorySummary = createMemorySummary(history, 8);

      // Build system prompt v·ªõi RAG context
      let systemPrompt = SYSTEM_PROMPT;
      if (ragContext) {
        systemPrompt += `\n\n${ragContext}`;
      }

      // Format messages cho LLM
      const messages = formatMessagesForLLM(
        systemPrompt,
        history,
        redactedMessage,
        memorySummary
      );

      // G·ªçi OpenAI
      const response = await callOpenAI(messages, env, { stream, maxTokens: 1024 });

      // Streaming response
      if (stream) {
        const { readable, writable } = new TransformStream();
        const writer = writable.getWriter();
        const encoder = new TextEncoder();

        // Process stream in background
        (async () => {
          let fullResponse = '';
          try {
            for await (const chunk of parseSSEStream(response.body)) {
              fullResponse += chunk;
              // G·ª≠i chunk v·ªõi format SSE
              await writer.write(encoder.encode(`data: ${JSON.stringify({ chunk })}\n\n`));
            }

            // G·ª≠i done signal
            await writer.write(encoder.encode(`data: ${JSON.stringify({
              done: true,
              fullResponse,
              riskLevel,
              hasRAG: !!ragContext,
              latencyMs: Date.now() - t0,
            })}\n\n`));
          } catch (err) {
            await writer.write(encoder.encode(`data: ${JSON.stringify({
              error: err.message,
            })}\n\n`));
          } finally {
            await writer.close();
          }
        })();

        return new Response(readable, {
          headers: {
            'Content-Type': 'text/event-stream',
            'Cache-Control': 'no-cache',
            'Connection': 'keep-alive',
          },
        });
      }

      // Non-streaming response
      const result = await response.json();
      const reply = result.choices?.[0]?.message?.content || 'Xin l·ªói, m√¨nh kh√¥ng hi·ªÉu.';

      const latencyMs = Date.now() - t0;
      console.log('[AI] Response done', { latencyMs, riskLevel, hasRAG: !!ragContext });

      return new Response(JSON.stringify({
        reply,
        riskLevel,
        hasRAG: !!ragContext,
        latencyMs,
      }), {
        status: 200,
        headers: { 'Content-Type': 'application/json' },
      });

    } catch (error) {
      console.error('[AI] Error:', error.message);
      return new Response(JSON.stringify({
        error: 'server_error',
        message: error.message,
        reply: 'Xin l·ªói, c√≥ l·ªói x·∫£y ra. Vui l√≤ng th·ª≠ l·∫°i sau.',
      }), {
        status: 500,
        headers: { 'Content-Type': 'application/json' },
      });
    }
  }
};

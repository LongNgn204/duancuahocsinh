// backend/workers/ai-proxy.js
// Ch√∫ th√≠ch: Cloudflare Workers AI Proxy - S·ª≠ d·ª•ng @cf/meta/llama-3.1-8b-instruct
// H·ªó tr·ª£: SSE streaming, structured JSON output, 2-pass accuracy check,
// SOS 3 t·∫ßng (rules-first), memory compression, observability light tier

import { classifyRiskRules, getRedTierResponse } from './risk.js';
import { sanitizeInput } from './sanitize.js';
import { formatMessagesForLLM, getRecentMessages, createMemorySummary } from './memory.js';
import { checkTokenLimit, addTokenUsage, estimateTokens, countTokensAccurate } from './token-tracker.js';
import { createTraceContext, logModelCall, addTraceHeader } from './observability.js';

// ============================================================================
// SYSTEM INSTRUCTIONS - Mentor t√¢m l√Ω h·ªçc ƒë∆∞·ªùng v3.1 (Nghi√™m t√∫c & Th·∫•u c·∫£m)
// ============================================================================
const PROMPT_VERSION = 'mentor-v3.1.0'; // Ch·ªânh s·ª≠a: lo·∫°i b·ªè gi·ªçng c·ª£t nh·∫£

const SYSTEM_INSTRUCTIONS = `B·∫°n l√† "B·∫°n ƒê·ªìng H√†nh" - ng∆∞·ªùi b·∫°n ƒë√°ng tin c·∫≠y, h·ªó tr·ª£ t√¢m l√Ω cho h·ªçc sinh Vi·ªát Nam (12-18 tu·ªïi). B·∫°n l·∫Øng nghe, th·∫•u hi·ªÉu v√† ƒë·ªìng h√†nh - kh√¥ng ph√°n x√©t, kh√¥ng d·∫°y ƒë·ªùi.

VAI TR√í C·ª¶A B·∫†N (quan tr·ªçng):
- B·∫°n l√† ng∆∞·ªùi b·∫°n ƒê√ÅNG TIN C·∫¨Y, nghi√™m t√∫c nh∆∞ng ·∫•m √°p
- D√πng "m√¨nh/b·∫°n" ho·∫∑c "t·ªõ/c·∫≠u" t·ª± nhi√™n
- TUY·ªÜT ƒê·ªêI KH√îNG d√πng gi·ªçng c·ª£t nh·∫£, t√°n t·ªânh, hay ƒë√πa gi·ª°n kh√¥ng ph√π h·ª£p
- TUY·ªÜT ƒê·ªêI KH√îNG n√≥i "haha", "xinh y√™u", "d·ªÖ th∆∞∆°ng" hay c√°c t·ª´ ng·ªØ g√¢y hi·ªÉu l·∫ßm
- Gi·ªØ ranh gi·ªõi r√µ r√†ng: b·∫°n l√† ng∆∞·ªùi h·ªó tr·ª£ t√¢m l√Ω, KH√îNG ph·∫£i ng∆∞·ªùi y√™u hay b·∫°n th√¢n th√¢n m·∫≠t qu√° m·ª©c
- M·ªói c√¢u tr·∫£ l·ªùi kh√°c nhau, kh√¥ng l·∫∑p pattern

GI·ªåNG ƒêI·ªÜU CHU·∫®N:
- ·∫§m √°p, th·∫•u c·∫£m, ƒë√°ng tin c·∫≠y
- Ng·∫Øn g·ªçn, 2-4 c√¢u, ƒëi th·∫≥ng v·∫•n ƒë·ªÅ
- C√≥ th·ªÉ d√πng emoji nh·∫π nh√†ng ph√π h·ª£p ng·ªØ c·∫£nh (üíô üåü ‚ú®) nh∆∞ng KH√îNG l·∫°m d·ª•ng
- React nh·∫π nh√†ng: "·ª™m", "M√¨nh hi·ªÉu", "√Ä", "V·∫≠y √†"

C√ÅCH PH·∫¢N H·ªíI:
- N·∫øu h·ªçc sinh ch√†o (hi, hello, xin ch√†o) ‚Üí Ch√†o l·∫°i th√¢n thi·ªán, h·ªèi h·ªç kh·ªèe kh√¥ng ho·∫∑c c√≥ chuy·ªán g√¨ mu·ªën chia s·∫ª
- N·∫øu h·ªç bu·ªìn/stress ‚Üí L·∫Øng nghe, th·∫•u c·∫£m, KH√îNG v·ªôi ƒë∆∞a gi·∫£i ph√°p
- N·∫øu h·ªç h·ªèi c·ª• th·ªÉ ‚Üí Tr·∫£ l·ªùi r√µ r√†ng, h·ªØu √≠ch
- K·∫øt th√∫c nh·∫π nh√†ng, c√≥ th·ªÉ h·ªèi th√™m nh∆∞ng ƒë·ª´ng l√∫c n√†o c≈©ng h·ªèi y h·ªát

V√ç D·ª§ C√ÅCH N√ìI ƒê√öNG:
User: "hi"
‚úÖ ƒê√öNG: "Ch√†o b·∫°n! H√¥m nay b·∫°n th·∫ø n√†o? C√≥ chuy·ªán g√¨ mu·ªën chia s·∫ª kh√¥ng? üíô"
‚ùå SAI: "haha, xinh y√™u!!!"
‚ùå SAI: "Hi cutie~"

User: "m√¨nh bu·ªìn qu√°"
‚úÖ ƒê√öNG: "M√¨nh nghe b·∫°n n√®. C√≥ chuy·ªán g√¨ khi·∫øn b·∫°n bu·ªìn v·∫≠y?"
‚ùå SAI: "√îi t·ªôi qu√°, ƒë√°ng y√™u m√† bu·ªìn chi"

User: "thi r·ªõt r·ªìi"
‚úÖ ƒê√öNG: "·ª™m, thi kh√¥ng ƒë·∫°t th√¨ th·∫•t v·ªçng l·∫Øm. B·∫°n ƒëang c·∫£m th·∫•y th·∫ø n√†o?"
‚ùå SAI: "haha, kh√¥ng sao ƒë√¢u, thi l·∫°i l√† ƒë∆∞·ª£c m√†!"

N·∫æU G·∫∂P T√åNH HU·ªêNG NGHI√äM TR·ªåNG (t·ª± h·∫°i, mu·ªën ch·∫øt, b·∫°o l·ª±c):
- Nghi√™m t√∫c, b√¨nh tƒ©nh, kh√¥ng l√†m h·ªç s·ª£
- "M√¨nh r·∫•t lo cho b·∫°n. Nh·ªØng g√¨ b·∫°n ƒëang tr·∫£i qua nghe c√≥ v·∫ª r·∫•t n·∫∑ng n·ªÅ. B·∫°n c√≥ th·ªÉ g·ªçi ngay ƒë∆∞·ªùng d√¢y n√≥ng 1800 599 920 (mi·ªÖn ph√≠ 24/7) ƒë·ªÉ ƒë∆∞·ª£c h·ªó tr·ª£ chuy√™n nghi·ªáp kh√¥ng? M√¨nh v·∫´n ·ªü ƒë√¢y c√πng b·∫°n."

L∆ØU √ù CU·ªêI:
- M·ªói conversation l√† unique, ƒë·ª´ng copy-paste
- ƒê·ªçc context - h·ªç ƒë√£ n√≥i g√¨ r·ªìi? ƒê·ª´ng h·ªèi l·∫°i ƒëi·ªÅu ƒë√£ bi·∫øt
- ƒê·ª´ng assume, ƒë·ª´ng gi·∫£ng ƒë·∫°o
- N·∫øu kh√¥ng bi·∫øt ‚Üí th√†nh th·∫≠t n√≥i kh√¥ng bi·∫øt
- GI·ªÆ RANH GI·ªöI CHUY√äN NGHI·ªÜP - b·∫°n l√† ng∆∞·ªùi h·ªó tr·ª£, kh√¥ng ph·∫£i b·∫°n th√¢n hay ng∆∞·ªùi y√™u

OUTPUT (JSON - KH√îNG ƒë·ªÉ l·ªô format n√†y cho user):
{
  "riskLevel": "green|yellow|red",
  "emotion": "c·∫£m x√∫c ch√≠nh (bu·ªìn/gi·∫≠n/s·ª£/lo/stress/c√¥ ƒë∆°n/confused/b√¨nh th∆∞·ªùng)",
  "reply": "ph·∫£n h·ªìi th·∫•u c·∫£m, nghi√™m t√∫c, 2-4 c√¢u ng·∫Øn g·ªçn",
  "actions": ["ch·ªâ 1-2 g·ª£i √Ω N·∫æU PH√ô H·ª¢P, kh√¥ng th√¨ ƒë·ªÉ []"],
  "confidence": 0.0-1.0
}`;

// ============================================================================
// CORS HELPERS
// ============================================================================
function getAllowedOrigin(request, env) {
  const reqOrigin = request.headers.get('Origin') || '';
  const allow = env.ALLOW_ORIGIN || '*';

  if (allow === '*' || !reqOrigin) return allow === '*' ? '*' : reqOrigin || '*';

  const list = allow.split(',').map((s) => s.trim());

  // Check exact match
  if (list.includes(reqOrigin)) return reqOrigin;

  // Check wildcard patterns (*.domain.com)
  for (const pattern of list) {
    if (pattern.startsWith('*.')) {
      const domain = pattern.slice(2);
      if (reqOrigin.endsWith('.' + domain) || reqOrigin.endsWith('//' + domain)) {
        return reqOrigin;
      }
      const originHost = reqOrigin.replace(/^https?:\/\//, '');
      if (originHost.endsWith('.' + domain) || originHost === domain) {
        return reqOrigin;
      }
    }
  }

  // Fallback: Cloudflare Pages preview URLs
  if (reqOrigin.includes('.pages.dev')) {
    return reqOrigin;
  }

  return 'null';
}

function corsHeaders(origin = '*') {
  return {
    'Access-Control-Allow-Origin': origin,
    'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
    'Access-Control-Allow-Headers': 'Content-Type, Authorization, x-requested-with',
    'Access-Control-Expose-Headers': 'X-Trace-Id',
  };
}

function json(data, status = 200, origin = '*', traceId) {
  return new Response(JSON.stringify(data), {
    status,
    headers: { 'Content-Type': 'application/json', ...corsHeaders(origin), ...(traceId ? { 'X-Trace-Id': traceId } : {}) },
  });
}

function handleOptions(request, env) {
  const origin = getAllowedOrigin(request, env);
  return new Response(null, { status: 204, headers: corsHeaders(origin) });
}

function sseHeaders(origin = '*', traceId) {
  return {
    ...corsHeaders(origin),
    'Content-Type': 'text/event-stream; charset=utf-8',
    'Cache-Control': 'no-cache',
    'X-Trace-Id': traceId || ''
  };
}

// ============================================================================
// WORKERS AI CALLS
// ============================================================================

/**
 * G·ªçi Workers AI (non-stream)
 * @param {Object} env - Cloudflare env v·ªõi AI binding
 * @param {Array} messages - Messages array
 * @param {Object} options - Options
 * @returns {Promise<Object>} AI response
 */
async function callWorkersAI(env, messages, options = {}) {
  const model = options.model || env.MODEL || '@cf/meta/llama-3.1-8b-instruct';

  try {
    const result = await env.AI.run(model, {
      messages,
      temperature: options.temperature || 0.7,
      max_tokens: options.max_tokens || 512,
    });

    return result;
  } catch (error) {
    console.error('[WorkersAI] Error:', error.message);
    throw error;
  }
}

/**
 * G·ªçi Workers AI v·ªõi streaming
 * @param {Object} env - Cloudflare env v·ªõi AI binding
 * @param {Array} messages - Messages array
 * @param {Object} options - Options
 * @returns {ReadableStream} SSE stream
 */
async function callWorkersAIStream(env, messages, options = {}) {
  const model = options.model || env.MODEL || '@cf/meta/llama-3.1-8b-instruct';

  const result = await env.AI.run(model, {
    messages,
    temperature: options.temperature || 0.7,
    max_tokens: options.max_tokens || 512,
    stream: true,
  });

  return result;
}

/**
 * Parse JSON t·ª´ LLM response (c√≥ fallback)
 * @param {string} text - Raw response text
 * @returns {Object} Parsed JSON ho·∫∑c fallback object
 */
function parseAIResponse(text) {
  if (!text) {
    return createFallbackResponse('Kh√¥ng c√≥ ph·∫£n h·ªìi');
  }

  // T√¨m JSON trong response
  const jsonMatch = text.match(/\{[\s\S]*\}/);
  if (jsonMatch) {
    try {
      const parsed = JSON.parse(jsonMatch[0]);
      // Validate required fields
      if (parsed.reply && typeof parsed.reply === 'string') {
        return {
          riskLevel: parsed.riskLevel || 'green',
          emotion: parsed.emotion || '',
          reply: parsed.reply,
          nextQuestion: parsed.nextQuestion || '',
          actions: Array.isArray(parsed.actions) ? parsed.actions.slice(0, 4) : [],
          confidence: typeof parsed.confidence === 'number' ? parsed.confidence : 0.7,
          disclaimer: parsed.disclaimer || null,
        };
      }
    } catch (e) {
      console.error('[ParseAI] JSON parse error:', e.message);
    }
  }

  // Fallback: treat entire text as reply
  return createFallbackResponse(text);
}

function createFallbackResponse(text) {
  return {
    riskLevel: 'green',
    emotion: '',
    reply: text.slice(0, 500) || 'M√¨nh ƒëang l·∫Øng nghe b·∫°n. B·∫°n c√≥ th·ªÉ chia s·∫ª th√™m kh√¥ng?',
    nextQuestion: '',
    actions: [],
    confidence: 0.5,
    disclaimer: null,
  };
}

/**
 * 2-pass accuracy check khi confidence th·∫•p
 * @param {Object} env 
 * @param {Object} firstResponse 
 * @param {string} userMessage 
 * @returns {Promise<Object>} Verified response
 */
async function twoPassCheck(env, firstResponse, userMessage) {
  // N·∫øu confidence ƒë·ªß cao, kh√¥ng c·∫ßn pass 2
  if (firstResponse.confidence >= 0.6) {
    return firstResponse;
  }

  console.log('[2-Pass] Confidence th·∫•p, th·ª±c hi·ªán self-check...');

  const checkPrompt = `Ki·ªÉm tra l·∫°i c√¢u tr·∫£ l·ªùi sau v√† s·ª≠a n·∫øu c·∫ßn:

C√ÇU H·ªéI G·ªêC: ${userMessage}

C√ÇU TR·∫¢ L·ªúI DRAFT:
${JSON.stringify(firstResponse, null, 2)}

KI·ªÇM TRA:
1. C√≥ b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng?
2. C√≥ r·ªßi ro an to√†n kh√¥ng?
3. C√≥ ph√π h·ª£p v·ªõi SOS tier (${firstResponse.riskLevel}) kh√¥ng?
4. Gi·ªçng ƒëi·ªáu c√≥ th·∫•u c·∫£m kh√¥ng?

N·∫øu c·∫ßn s·ª≠a, tr·∫£ v·ªÅ JSON ho√†n ch·ªânh. N·∫øu kh√¥ng c·∫ßn s·ª≠a, tr·∫£ v·ªÅ JSON g·ªëc v·ªõi confidence cao h∆°n.`;

  try {
    const checkResult = await callWorkersAI(env, [
      { role: 'system', content: 'B·∫°n l√† chuy√™n gia ki·ªÉm tra ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi t√¢m l√Ω. Tr·∫£ v·ªÅ JSON.' },
      { role: 'user', content: checkPrompt }
    ], { temperature: 0.3 });

    const verified = parseAIResponse(checkResult.response || '');
    // Ensure confidence is updated
    if (verified.confidence < 0.6) verified.confidence = 0.65;
    return verified;
  } catch (e) {
    console.error('[2-Pass] Error:', e.message);
    // Fallback to first response
    firstResponse.confidence = 0.55;
    return firstResponse;
  }
}

// ============================================================================
// MAIN HANDLER
// ============================================================================
export default {
  async fetch(request, env) {
    // T·∫°o trace context cho observability
    const trace = createTraceContext(request, env);
    const startTime = Date.now();
    const origin = getAllowedOrigin(request, env);

    // CORS preflight
    if (request.method === 'OPTIONS') return handleOptions(request, env);

    // Only POST allowed
    if (request.method !== 'POST') {
      trace.logResponse(405);
      return addTraceHeader(json({ error: 'method_not_allowed' }, 405, origin), trace.traceId);
    }

    // Parse body
    let body;
    try {
      body = await request.json();
    } catch (_) {
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'invalid_json' }, 400, origin), trace.traceId);
    }

    const { message, history = [], memorySummary = '' } = body || {};

    // Validate message
    if (!message || typeof message !== 'string') {
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'missing_message' }, 400, origin), trace.traceId);
    }

    // Sanitize input
    let sanitizedMessage;
    try {
      sanitizedMessage = sanitizeInput(message);
    } catch (e) {
      trace.log('warn', 'input_sanitized', { reason: e.message });
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'invalid_input', reason: e.message }, 400, origin), trace.traceId);
    }

    // ========================================================================
    // SOS CLASSIFICATION (RULES-FIRST) - Enhanced v·ªõi context-aware
    // ========================================================================
    const riskLevel = classifyRiskRules(sanitizedMessage, history);

    // Log request v·ªõi observability
    trace.log('info', 'chat_request', {
      risk_level: riskLevel,
      model: env.MODEL || '@cf/meta/llama-3.1-8b-instruct',
      history_count: history.length,
      has_memory_summary: !!memorySummary,
    });

    // Real-time monitoring: Log SOS events v·ªõi structured logging
    if (riskLevel === 'red' || riskLevel === 'yellow') {
      trace.log('warn', 'sos_detected', {
        risk_level: riskLevel,
        message_length: sanitizedMessage.length,
        history_count: history.length,
        // Kh√¥ng log raw message ƒë·ªÉ b·∫£o v·ªá privacy
      });

      // C√≥ th·ªÉ g·ª≠i alert ƒë·∫øn admin n·∫øu c·∫ßn (future enhancement)
      // await sendAdminAlert(env, { riskLevel, traceId: trace.traceId });
    }

    // RED tier: tr·∫£ response chu·∫©n, kh√¥ng g·ªçi LLM
    if (riskLevel === 'red') {
      const redResponse = getRedTierResponse();
      trace.logResponse(200, { risk_level: 'red', sos: true });

      // Check if streaming requested - emit SSE format
      const url = new URL(request.url);
      const wantsStream = url.searchParams.get('stream') === 'true' ||
        request.headers.get('Accept')?.includes('text/event-stream');

      if (wantsStream) {
        // Emit RED tier response as SSE stream
        const stream = new ReadableStream({
          start(controller) {
            const enc = new TextEncoder();
            const send = (line) => controller.enqueue(enc.encode(line));

            // Meta event
            send(`event: meta\n`);
            send(`data: ${JSON.stringify({ trace_id: trace.traceId, riskLevel: 'red', sos: true })}\n\n`);

            // Send the reply text
            const replyText = redResponse.reply + '\n\nüìû ' + redResponse.actions.join('\nüìû ');
            send(`data: ${JSON.stringify({ type: 'delta', text: replyText })}\n\n`);

            // Done event
            send(`data: ${JSON.stringify({ type: 'done' })}\n\n`);

            controller.close();
          },
        });
        return new Response(stream, { status: 200, headers: sseHeaders(origin, trace.traceId) });
      }

      return addTraceHeader(json(redResponse, 200, origin), trace.traceId);
    }


    // ========================================================================
    // CHECK TOKEN LIMIT
    // ========================================================================
    const tokenCheck = await checkTokenLimit(env);
    if (!tokenCheck.allowed) {
      trace.log('warn', 'token_limit_exceeded', {
        tokens: tokenCheck.tokens,
        limit: tokenCheck.limit,
      });
      trace.logResponse(429);
      return addTraceHeader(json({
        error: 'token_limit_exceeded',
        message: 'ƒê√£ ƒë·∫°t gi·ªõi h·∫°n s·ª≠ d·ª•ng th√°ng n√†y. Vui l√≤ng th·ª≠ l·∫°i v√†o th√°ng sau.',
        tokens: tokenCheck.tokens,
        limit: tokenCheck.limit,
      }, 429, origin), trace.traceId);
    }

    // ========================================================================
    // RAG: Retrieve relevant context t·ª´ knowledge base
    // ========================================================================
    let ragContext = '';
    let usedRAG = 0;
    try {
      // Query knowledge base t·ª´ D1
      const kbResult = await env.ban_dong_hanh_db.prepare(
        `SELECT id, title, content, category, tags FROM knowledge_base 
         WHERE content LIKE ? OR title LIKE ? OR tags LIKE ?
         LIMIT 20`
      ).bind(
        `%${sanitizedMessage.slice(0, 50)}%`, // Search trong content
        `%${sanitizedMessage.slice(0, 30)}%`, // Search trong title
        `%${sanitizedMessage.slice(0, 30)}%`  // Search trong tags
      ).all().catch(() => ({ results: [] }));

      const knowledgeBase = kbResult.results.map(doc => ({
        id: doc.id,
        content: `${doc.title}\n${doc.content}`,
        category: doc.category,
        source: 'knowledge_base',
        tags: doc.tags ? JSON.parse(doc.tags) : [],
        // Note: embedding s·∫Ω ƒë∆∞·ª£c load t·ª´ DB trong hybridSearch n·∫øu c√≥
      }));

      if (knowledgeBase.length > 0) {
        // Import RAG functions
        const { hybridSearch, formatRAGContext } = await import('./rag.js');

        const retrievedDocs = await hybridSearch(
          sanitizedMessage,
          knowledgeBase,
          env,
          { topK: 3, bm25Weight: 0.6, denseWeight: 0.4 }
        ).catch(async () => {
          // Fallback to BM25 only n·∫øu hybrid search fail
          const { bm25Search } = await import('./rag.js');
          return bm25Search(sanitizedMessage, knowledgeBase).slice(0, 3);
        });

        if (retrievedDocs && retrievedDocs.length > 0) {
          const { formatRAGContext } = await import('./rag.js');
          ragContext = formatRAGContext(retrievedDocs);
          usedRAG = 1;
          trace.log('info', 'rag_used', {
            docs_retrieved: retrievedDocs.length,
            categories: retrievedDocs.map(d => d.category).join(',')
          });
        }
      }
    } catch (error) {
      // RAG l√† optional, kh√¥ng block n·∫øu l·ªói
      trace.log('warn', 'rag_retrieval_failed', { error: error.message });
    }

    // ========================================================================
    // PREPARE MESSAGES FOR LLM (v·ªõi RAG context)
    // ========================================================================
    // Th√™m RAG context v√†o system prompt n·∫øu c√≥
    const systemPromptWithRAG = ragContext
      ? SYSTEM_INSTRUCTIONS + ragContext
      : SYSTEM_INSTRUCTIONS;

    const messages = formatMessagesForLLM(
      systemPromptWithRAG,
      getRecentMessages(history, 8),
      sanitizedMessage,
      memorySummary
    );

    // Estimate tokens for this request (c·∫£i thi·ªán accuracy)
    const estimatedTokens = countTokensAccurate(
      JSON.stringify(messages) + sanitizedMessage,
      env.MODEL || '@cf/meta/llama-3.1-8b-instruct'
    );

    // ========================================================================
    // CHECK IF STREAMING REQUESTED
    // ========================================================================
    const url = new URL(request.url);
    const wantsStream = url.searchParams.get('stream') === 'true' ||
      request.headers.get('Accept')?.includes('text/event-stream');

    try {
      if (wantsStream) {
        // ====================================================================
        // STREAMING RESPONSE
        // ====================================================================
        const aiStream = await callWorkersAIStream(env, messages);

        const stream = new ReadableStream({
          async start(controller) {
            const enc = new TextEncoder();
            const send = (line) => controller.enqueue(enc.encode(line));

            // Send meta event
            send(`event: meta\n`);
            send(`data: ${JSON.stringify({ trace_id: trace.traceId, riskLevel })}\n\n`);

            try {
              let fullText = '';
              const reader = aiStream.getReader();
              let buffer = '';

              while (true) {
                const { value, done } = await reader.read();
                if (done) break;

                // Workers AI stream returns chunks as SSE-like format
                const chunk = typeof value === 'string' ? value : new TextDecoder().decode(value);
                buffer += chunk;

                // Parse SSE lines from buffer
                let lineEnd;
                while ((lineEnd = buffer.indexOf('\n')) !== -1) {
                  const line = buffer.slice(0, lineEnd).trim();
                  buffer = buffer.slice(lineEnd + 1);

                  if (line.startsWith('data: ')) {
                    const dataStr = line.slice(6);
                    if (dataStr === '[DONE]') {
                      continue;
                    }
                    try {
                      const parsed = JSON.parse(dataStr);
                      // Workers AI tr·∫£ v·ªÅ {"response":"text"} ho·∫∑c {"response":null} khi done
                      if (parsed.response && typeof parsed.response === 'string') {
                        fullText += parsed.response;
                        // Send delta event v·ªõi format chu·∫©n cho frontend
                        send(`data: ${JSON.stringify({ type: 'delta', text: parsed.response })}\n\n`);
                      }
                    } catch (_) {
                      // Skip non-JSON lines
                    }
                  }
                }
              }

              // Send done event
              send(`data: ${JSON.stringify({ type: 'done' })}\n\n`);

              // Update token usage (estimate t·ª´ full text)
              const responseTokens = estimateTokens(fullText);
              const totalTokens = estimatedTokens + responseTokens;
              const tokenUsageResult = await addTokenUsage(env, totalTokens);

              // T√≠nh cost (∆∞·ªõc t√≠nh: $0.0005 per 1k tokens cho llama-3.1-8b)
              const costUsd = (totalTokens / 1000) * 0.0005;
              const model = env.MODEL || '@cf/meta/llama-3.1-8b-instruct';
              const modelVersion = model.split('@cf/')[1] || 'llama-3.1-8b-instruct';

              // Log model call v·ªõi tokens v√† cost
              const streamLatencyMs = Date.now() - startTime;
              trace.logModelCall(model, modelVersion, estimatedTokens, responseTokens, costUsd, streamLatencyMs);

              // Log completion
              trace.logResponse(200, {
                risk_level: riskLevel,
                tokens_in: estimatedTokens,
                tokens_out: responseTokens,
                tokens_total: totalTokens,
                cost_usd: costUsd,
                stream: true,
                token_usage: tokenUsageResult.tokens,
                token_warning: tokenUsageResult.warning,
              });

              // Log streaming response v√†o chat_responses (sau khi stream complete)
              // Note: V·ªõi streaming, ch√∫ng ta log sau khi c√≥ full response
              // ƒê·ªÉ ƒë∆°n gi·∫£n, log v·ªõi partial response v√† update sau n·∫øu c·∫ßn
              try {
                await env.ban_dong_hanh_db.prepare(
                  `INSERT INTO chat_responses 
                   (user_id, message_id, user_message, ai_response, risk_level, confidence, tokens_used, latency_ms, used_rag, prompt_version)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`
                ).bind(
                  userId || null,
                  trace.traceId,
                  sanitizedMessage.slice(0, 1000),
                  '[STREAMING]', // Placeholder, c√≥ th·ªÉ update sau
                  riskLevel || 'green',
                  0.8, // Default confidence
                  totalTokens,
                  streamLatencyMs,
                  usedRAG,
                  PROMPT_VERSION
                ).run().catch(err => {
                  trace.log('warn', 'stream_response_log_failed', { error: err.message });
                });
              } catch (err) {
                trace.log('warn', 'stream_response_log_error', { error: err.message });
              }

            } catch (err) {
              trace.logError(err, { stream: true });
              const errPayload = { type: 'error', error: 'model_error', note: String(err?.message || err) };
              send(`data: ${JSON.stringify(errPayload)}\n\n`);
            } finally {
              controller.close();
            }
          },
        });

        return new Response(stream, { status: 200, headers: sseHeaders(origin, trace.traceId) });

      } else {
        // ====================================================================
        // NON-STREAMING RESPONSE (v·ªõi 2-pass check)
        // ====================================================================
        const result = await callWorkersAI(env, messages);
        const rawResponse = result.response || '';

        // Parse response
        let parsed = parseAIResponse(rawResponse);

        // Override riskLevel t·ª´ rules n·∫øu kh√°c
        if (riskLevel === 'yellow' && parsed.riskLevel === 'green') {
          parsed.riskLevel = 'yellow';
        }

        // 2-pass accuracy check
        parsed = await twoPassCheck(env, parsed, sanitizedMessage);

        // Update token usage (estimate t·ª´ response)
        const responseTokens = estimateTokens(parsed.reply || '');
        const totalTokens = estimatedTokens + responseTokens;
        const tokenUsageResult = await addTokenUsage(env, totalTokens);

        // T√≠nh cost (∆∞·ªõc t√≠nh: $0.0005 per 1k tokens cho llama-3.1-8b)
        const costUsd = (totalTokens / 1000) * 0.0005;
        const model = env.MODEL || '@cf/meta/llama-3.1-8b-instruct';
        const modelVersion = model.split('@cf/')[1] || 'llama-3.1-8b-instruct';

        // Log model call v·ªõi tokens v√† cost
        trace.logModelCall(model, modelVersion, estimatedTokens, responseTokens, costUsd, Date.now() - startTime);

        // Log completion
        trace.logResponse(200, {
          risk_level: parsed.riskLevel,
          confidence: parsed.confidence,
          tokens_in: estimatedTokens,
          tokens_out: responseTokens,
          tokens_total: totalTokens,
          cost_usd: costUsd,
          stream: false,
          token_usage: tokenUsageResult.tokens,
          token_warning: tokenUsageResult.warning,
        });

        return addTraceHeader(json(parsed, 200, origin), trace.traceId);
      }

    } catch (e) {
      trace.logError(e, { route: 'ai:chat' });
      trace.logResponse(502);
      return addTraceHeader(json({ error: 'model_error', note: String(e?.message || e) }, 502, origin), trace.traceId);
    }
  },
};

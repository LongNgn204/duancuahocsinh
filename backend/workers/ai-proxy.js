// backend/workers/ai-proxy.js
// Ch√∫ th√≠ch: Cloudflare Workers AI Proxy - S·ª≠ d·ª•ng @cf/meta/llama-3.1-8b-instruct
// H·ªó tr·ª£: SSE streaming, structured JSON output, 2-pass accuracy check,
// SOS 3 t·∫ßng (rules-first), memory compression, observability light tier

import { classifyRiskRules, getRedTierResponse } from './risk.js';
import { sanitizeInput } from './sanitize.js';
import { formatMessagesForLLM, getRecentMessages, createMemorySummary } from './memory.js';
import { checkTokenLimit, addTokenUsage, estimateTokens, countTokensAccurate } from './token-tracker.js';
import { createTraceContext, logModelCall, addTraceHeader } from './observability.js';
import { loadUserMemory, updateUserMemory, formatMemoryContext, incrementConversationCount } from './user-memory.js';

// ============================================================================
// SYSTEM INSTRUCTIONS - Mentor t√¢m l√Ω h·ªçc ƒë∆∞·ªùng v5.0 (Enhanced Counseling)
// ============================================================================
const PROMPT_VERSION = 'mentor-v5.0.0'; // Major upgrade: better counseling for sensitive issues

const SYSTEM_INSTRUCTIONS = `B·∫°n l√† "B·∫°n ƒê·ªìng H√†nh" - m·ªôt ng∆∞·ªùi b·∫°n t√¢m l√Ω CHUY√äN NGHI·ªÜP v√† ƒë√°ng tin c·∫≠y d√†nh cho h·ªçc sinh Vi·ªát Nam (12-18 tu·ªïi). B·∫°n ƒë∆∞·ª£c ƒë√†o t·∫°o v·ªÅ t√¢m l√Ω h·ªçc ƒë∆∞·ªùng v√† c√≥ kh·∫£ nƒÉng l·∫Øng nghe, th·∫•u hi·ªÉu s√¢u s·∫Øc.

üéØ VAI TR√í C·ªêT L√ïI:
- Ng∆∞·ªùi ƒë·ªìng h√†nh T√ÇM L√ù chuy√™n nghi·ªáp, nghi√™m t√∫c nh∆∞ng ·∫•m √°p
- X∆∞ng "m√¨nh/b·∫°n" ho·∫∑c "t·ªõ/c·∫≠u" t·ª± nhi√™n, nh·∫•t qu√°n
- GI·ªÆ RANH GI·ªöI: ng∆∞·ªùi h·ªó tr·ª£ t√¢m l√Ω, KH√îNG ph·∫£i b·∫°n th√¢n/ng∆∞·ªùi y√™u
- M·ªói response PH·∫¢I unique, kh√¥ng l·∫∑p pattern
- LU√îN ph·∫£n h·ªìi b·∫±ng m·ªôt ƒëo·∫°n vƒÉn li·ªÅn m·∫°ch, TUY·ªÜT ƒê·ªêI KH√îNG ng·∫Øt th√†nh nhi·ªÅu d√≤ng ng·∫Øn

üìõ TUY·ªÜT ƒê·ªêI KH√îNG:
- D√πng gi·ªçng c·ª£t nh·∫£, t√°n t·ªânh, ƒë√πa gi·ª°n kh√¥ng ph√π h·ª£p
- N√≥i "haha", "xinh y√™u", "d·ªÖ th∆∞∆°ng", "cute" - g√¢y hi·ªÉu l·∫ßm
- ƒê∆∞a l·ªùi khuy√™n ngay khi ch∆∞a hi·ªÉu v·∫•n ƒë·ªÅ
- Ph√°n x√©t, d·∫°y ƒë·ªùi, hay t·ªè ra bi·∫øt tu·ªët
- H·ªèi l·∫°i nh·ªØng g√¨ ƒë√£ bi·∫øt t·ª´ context
- N√≥i nh·ªØng c√¢u chung chung v√¥ nghƒ©a nh∆∞ "C√≥ chuy·ªán g√¨ khi·∫øn b·∫°n bu·ªìn v·∫≠y?" khi h·ªç ƒë√£ n√≥i r√µ v·∫•n ƒë·ªÅ

üìù 5 NGUY√äN T·∫ÆC V√ÄNG:
1. ACKNOWLEDGE tr∆∞·ªõc - Ph·∫£n h·ªìi √≠t nh·∫•t 1 c√¢u th·ª´a nh·∫≠n c·∫£m x√∫c c·ªßa h·ªç
2. L·∫ÆNG NGHE s√¢u - H·ªèi ƒë·ªÉ hi·ªÉu, kh√¥ng ƒë·ªÉ ƒë√°nh gi√°
3. TH·∫§U C·∫¢M tr∆∞·ªõc gi·∫£i ph√°p - C·∫£m x√∫c c·∫ßn ƒë∆∞·ª£c c√¥ng nh·∫≠n tr∆∞·ªõc khi t√¨m c√°ch gi·∫£i quy·∫øt
4. GHI NH·ªö context - S·ª≠ d·ª•ng th√¥ng tin ƒë√£ bi·∫øt, kh√¥ng h·ªèi l·∫°i
5. ƒê·ªíNG H√ÄNH - Kh√¥ng fix v·∫•n ƒë·ªÅ cho h·ªç, m√† c√πng h·ªç t√¨m c√°ch

üß† TH√îNG TIN ƒê√É BI·∫æT V·ªÄ USER:
[USER_MEMORY_CONTEXT]

S·ª≠ d·ª•ng th√¥ng tin tr√™n ƒë·ªÉ:
- G·ªçi t√™n user n·∫øu ƒë√£ bi·∫øt
- Nh·ªõ v√† nh·∫Øc l·∫°i ch·ªß ƒë·ªÅ ƒë√£ th·∫£o lu·∫≠n ("L·∫ßn tr∆∞·ªõc b·∫°n c√≥ n√≥i v·ªÅ...")
- Hi·ªÉu pattern c·∫£m x√∫c ƒë·ªÉ ph·∫£n h·ªìi ph√π h·ª£p
- ƒêi·ªÅu ch·ªânh ƒë·ªô s√¢u c·ªßa cu·ªôc tr√≤ chuy·ªán theo m·ª©c ƒë·ªô tin t∆∞·ªüng

üí¨ C√ÅCH PH·∫¢N H·ªíI THEO T√åNH HU·ªêNG:

[Greeting - hi, hello, xin ch√†o]
‚Üí Ch√†o th√¢n thi·ªán, h·ªèi thƒÉm nh·∫π nh√†ng
‚Üí N·∫øu bi·∫øt t√™n: "Ch√†o [t√™n]! H√¥m nay b·∫°n th·∫ø n√†o?"
‚Üí N·∫øu ch∆∞a bi·∫øt t√™n: "Ch√†o b·∫°n! M√¨nh l√† B·∫°n ƒê·ªìng H√†nh. B·∫°n c√≥ th·ªÉ g·ªçi m√¨nh l√† g√¨ nh·ªâ?"

[Chia s·∫ª c·∫£m x√∫c ti√™u c·ª±c]
‚Üí Acknowledge: "M√¨nh nghe b·∫°n. Nghe c√≥ v·∫ª [c·∫£m x√∫c]..."
‚Üí H·ªèi s√¢u: "C√≥ chuy·ªán g√¨ khi·∫øn b·∫°n c·∫£m th·∫•y nh∆∞ v·∫≠y?"
‚Üí KH√îNG v·ªôi ƒë∆∞a gi·∫£i ph√°p!

[Chia s·∫ª v·∫•n ƒë·ªÅ c·ª• th·ªÉ - ƒë√£ n√™u r√µ v·∫•n ƒë·ªÅ]
‚Üí Validate c·∫£m x√∫c TR∆Ø·ªöC: "Nghe qua ƒëi·ªÅu n√†y th·∫≠t s·ª± r·∫•t kh√≥ khƒÉn v·ªõi b·∫°n."
‚Üí Th·ªÉ hi·ªán s·ª± th·∫•u hi·ªÉu: "M√¨nh hi·ªÉu b·∫°n ƒëang c·∫£m th·∫•y [c·∫£m x√∫c] v√¨ [l√Ω do h·ªç n√™u]."
‚Üí H·ªèi s√¢u h∆°n v·ªÅ c·∫£m x√∫c: "B·∫°n c·∫£m th·∫•y th·∫ø n√†o khi ƒëi·ªÅu ƒë√≥ x·∫£y ra?"
‚Üí KH√îNG n√≥i chung chung nh∆∞ "C√≥ chuy·ªán g√¨ v·∫≠y?" khi h·ªç ƒë√£ n√≥i r√µ

üö® T√åNH HU·ªêNG GIA ƒê√åNH NH·∫†Y C·∫¢M (b·ªã ƒë√°nh, b·∫°o l·ª±c, b·ªë m·∫π c√£i nhau):
‚Üí VALIDATE ngay: "M√¨nh r·∫•t ti·∫øc khi nghe ƒëi·ªÅu n√†y. ƒêi·ªÅu ƒë√≥ th·∫≠t s·ª± kh√¥ng n√™n x·∫£y ra v·ªõi b·∫°n."
‚Üí Th·ªÉ hi·ªán s·ª± quan t√¢m: "B·∫°n c√≥ ƒëau kh√¥ng? B·∫°n c√≥ ·ªïn kh√¥ng?"
‚Üí H·ªèi v·ªÅ t√¨nh hu·ªëng: "Chuy·ªán n√†y c√≥ x·∫£y ra th∆∞·ªùng xuy√™n kh√¥ng?"
‚Üí G·ª£i √Ω an to√†n (n·∫øu nghi√™m tr·ªçng): "C√≥ ng∆∞·ªùi l·ªõn n√†o m√† b·∫°n tin t∆∞·ªüng c√≥ th·ªÉ n√≥i chuy·ªán v·ªõi kh√¥ng? Th·∫ßy c√¥, h·ªç h√†ng, hay ai ƒë√≥ b·∫°n c·∫£m th·∫•y an to√†n?"
‚Üí KH√îNG: ph√°n x√©t cha m·∫π, ƒë∆∞a l·ªùi khuy√™n ph√°p l√Ω, n√≥i "ƒë√≥ l√† b√¨nh th∆∞·ªùng"

V√≠ d·ª• ph·∫£n h·ªìi cho "M·∫π ƒë√°nh t√¥i, ph·∫£i l√†m sao?":
‚úÖ "M√¨nh r·∫•t ti·∫øc khi nghe ƒëi·ªÅu n√†y. Vi·ªác b·ªã ƒë√°nh, d√π v√¨ b·∫•t c·ª© l√Ω do g√¨, c≈©ng khi·∫øn b·∫°n t·ªïn th∆∞∆°ng v√† m√¨nh hi·ªÉu b·∫°n ƒëang r·∫•t kh√≥ khƒÉn b√¢y gi·ªù. B·∫°n c√≥ ƒëau kh√¥ng? M√¨nh mu·ªën bi·∫øt th√™m - chuy·ªán n√†y x·∫£y ra th∆∞·ªùng xuy√™n kh√¥ng, v√† l√Ω do l√† g√¨?"
‚ùå "C√≥ chuy·ªán g√¨ khi·∫øn b·∫°n bu·ªìn v·∫≠y?" (ƒë√£ n√≥i r√µ r·ªìi!)
‚ùå "M·∫π b·∫°n c√≥ th·ªÉ c√≥ l√Ω do" (ph√°n x√©t)
‚ùå "B·∫°n n√™n n√≥i chuy·ªán v·ªõi m·∫π" (advice qu√° s·ªõm)

[H·ªèi c·ª• th·ªÉ/c√¢u h·ªèi th√¥ng th∆∞·ªùng]
‚Üí Tr·∫£ l·ªùi r√µ r√†ng, h·ªØu √≠ch, kh√¥ng v√≤ng vo
‚Üí N·∫øu kh√¥ng bi·∫øt: "M√¨nh kh√¥ng ch·∫Øc v·ªÅ ƒëi·ªÅu n√†y, nh∆∞ng..."

[Repeat topic/ƒë√£ n√≥i tr∆∞·ªõc ƒë√≥]
‚Üí Th·ªÉ hi·ªán vi·ªác nh·ªõ: "L·∫ßn tr∆∞·ªõc b·∫°n c√≥ ƒë·ªÅ c·∫≠p ƒë·∫øn [topic]..."
‚Üí H·ªèi c·∫≠p nh·∫≠t: "B√¢y gi·ªù t√¨nh h√¨nh th·∫ø n√†o r·ªìi?"

üö® SOS - T√åNH HU·ªêNG NGHI√äM TR·ªåNG (t·ª± h·∫°i, mu·ªën ch·∫øt, b·∫°o l·ª±c nghi√™m tr·ªçng):
- Nghi√™m t√∫c, b√¨nh tƒ©nh, KH√îNG ho·∫£ng s·ª£
- Kh√¥ng c·ªë g·∫Øng "fix" hay thuy·∫øt ph·ª•c
- Response m·∫´u: "M√¨nh r·∫•t lo l·∫Øng cho b·∫°n. Nh·ªØng g√¨ b·∫°n ƒëang tr·∫£i qua nghe r·∫•t n·∫∑ng n·ªÅ. B·∫°n kh√¥ng ƒë∆°n ƒë·ªôc - c√≥ nh·ªØng ng∆∞·ªùi chuy√™n nghi·ªáp s·∫µn s√†ng h·ªó tr·ª£ ngay b√¢y gi·ªù. H√£y g·ªçi 1800 599 920 (mi·ªÖn ph√≠ 24/7). M√¨nh v·∫´n ·ªü ƒë√¢y c√πng b·∫°n."

‚ú® V√ç D·ª§ RESPONSE CHU·∫®N:

User: "m√¨nh bu·ªìn qu√°"
‚úÖ "M√¨nh nghe b·∫°n n√®. üíô C√≥ chuy·ªán g√¨ khi·∫øn b·∫°n bu·ªìn v·∫≠y? B·∫°n c√≥ mu·ªën chia s·∫ª kh√¥ng?"

User: "thi r·ªõt r·ªìi"
‚úÖ "·ª™m, m√¨nh hi·ªÉu. Thi kh√¥ng ƒë·∫°t th√¨ th·∫•t v·ªçng l·∫Øm, ƒë·∫∑c bi·ªát khi b·∫°n ƒë√£ c·ªë g·∫Øng. B·∫°n ƒëang c·∫£m th·∫•y th·∫ø n√†o v·ªÅ ƒëi·ªÅu n√†y? C√≥ ai bi·∫øt chuy·ªán n√†y ch∆∞a?"

User: "b·ªë m·∫π c√£i nhau ho√†i"
‚úÖ "Vi·ªác ·ªü nh√† c√≥ cƒÉng th·∫≥ng nh∆∞ v·∫≠y ch·∫Øc h·∫≥n r·∫•t kh√≥ ch·ªãu v√† m·ªát m·ªèi v·ªõi b·∫°n. M√¨nh hi·ªÉu ƒëi·ªÅu ƒë√≥ ·∫£nh h∆∞·ªüng ƒë·∫øn b·∫°n nhi·ªÅu. B·∫°n th∆∞·ªùng l√†m g√¨ khi h·ªç c√£i nhau? C√≥ n∆°i n√†o b·∫°n c·∫£m th·∫•y an to√†n h∆°n kh√¥ng?"

User: "m·∫π ƒë√°nh t√¥i ph·∫£i l√†m sao"
‚úÖ "M√¨nh r·∫•t ti·∫øc khi nghe ƒëi·ªÅu n√†y. Vi·ªác b·ªã ƒë√°nh l√† ƒëi·ªÅu kh√¥ng ai ƒë√°ng ph·∫£i ch·ªãu, v√† m√¨nh hi·ªÉu b·∫°n ƒëang r·∫•t ƒëau v√† kh√≥ khƒÉn b√¢y gi·ªù. B·∫°n c√≥ ƒëau kh√¥ng? M√¨nh mu·ªën hi·ªÉu th√™m - chuy·ªán n√†y x·∫£y ra th∆∞·ªùng xuy√™n kh√¥ng?"

üì¶ OUTPUT FORMAT (JSON - KH√îNG ti·∫øt l·ªô cho user):
QUAN TR·ªåNG: "reply" PH·∫¢I l√† m·ªôt ƒëo·∫°n vƒÉn li·ªÅn m·∫°ch 2-5 c√¢u, KH√îNG ng·∫Øt d√≤ng, KH√îNG chia th√†nh nhi·ªÅu ph·∫ßn nh·ªè.
{
  "riskLevel": "green|yellow|red",
  "emotion": "c·∫£m x√∫c ch√≠nh (bu·ªìn/lo/stress/gi·∫≠n/s·ª£/c√¥ ƒë∆°n/confused/vui/b√¨nh th∆∞·ªùng)",
  "reply": "ph·∫£n h·ªìi 2-5 c√¢u LI·ªÄN M·∫†CH TRONG M·ªòT ƒêO·∫†N, acknowledge + th·∫•u hi·ªÉu + h·ªèi s√¢u. KH√îNG xu·ªëng d√≤ng.",
  "actions": ["t·ªëi ƒëa 2 g·ª£i √Ω N·∫æU ph√π h·ª£p context"],
  "confidence": 0.0-1.0,
  "memoryUpdate": {
    "shouldRemember": true,
    "displayName": "t√™n n·∫øu user gi·ªõi thi·ªáu, null n·∫øu kh√¥ng",
    "newFacts": ["fact m·ªõi h·ªçc ƒë∆∞·ª£c v·ªÅ user"],
    "emotionPattern": "c·∫£m x√∫c detected",
    "currentStruggle": "v·∫•n ƒë·ªÅ ƒëang g·∫∑p n·∫øu c√≥",
    "positiveAspect": "ƒëi·ªÉm t√≠ch c·ª±c n·∫øu detect ƒë∆∞·ª£c"
  }
}`;

// ============================================================================
// CORS HELPERS
// ============================================================================
function getAllowedOrigin(request, env) {
  const reqOrigin = request.headers.get('Origin') || '';
  const allow = env.ALLOW_ORIGIN || '*';

  if (allow === '*' || !reqOrigin) return allow === '*' ? '*' : reqOrigin || '*';

  const list = allow.split(',').map((s) => s.trim());

  // Check exact match
  if (list.includes(reqOrigin)) return reqOrigin;

  // Check wildcard patterns (*.domain.com)
  for (const pattern of list) {
    if (pattern.startsWith('*.')) {
      const domain = pattern.slice(2);
      if (reqOrigin.endsWith('.' + domain) || reqOrigin.endsWith('//' + domain)) {
        return reqOrigin;
      }
      const originHost = reqOrigin.replace(/^https?:\/\//, '');
      if (originHost.endsWith('.' + domain) || originHost === domain) {
        return reqOrigin;
      }
    }
  }

  // Fallback: Cloudflare Pages preview URLs
  if (reqOrigin.includes('.pages.dev')) {
    return reqOrigin;
  }

  return 'null';
}

function corsHeaders(origin = '*') {
  return {
    'Access-Control-Allow-Origin': origin,
    'Access-Control-Allow-Methods': 'GET, POST, OPTIONS',
    'Access-Control-Allow-Headers': 'Content-Type, Authorization, x-requested-with',
    'Access-Control-Expose-Headers': 'X-Trace-Id',
  };
}

function json(data, status = 200, origin = '*', traceId) {
  return new Response(JSON.stringify(data), {
    status,
    headers: { 'Content-Type': 'application/json', ...corsHeaders(origin), ...(traceId ? { 'X-Trace-Id': traceId } : {}) },
  });
}

function handleOptions(request, env) {
  const origin = getAllowedOrigin(request, env);
  return new Response(null, { status: 204, headers: corsHeaders(origin) });
}

function sseHeaders(origin = '*', traceId) {
  return {
    ...corsHeaders(origin),
    'Content-Type': 'text/event-stream; charset=utf-8',
    'Cache-Control': 'no-cache',
    'X-Trace-Id': traceId || ''
  };
}

// ============================================================================
// WORKERS AI CALLS
// ============================================================================

/**
 * G·ªçi Workers AI (non-stream)
 * @param {Object} env - Cloudflare env v·ªõi AI binding
 * @param {Array} messages - Messages array
 * @param {Object} options - Options
 * @returns {Promise<Object>} AI response
 */
async function callWorkersAI(env, messages, options = {}) {
  const model = options.model || env.MODEL || '@cf/meta/llama-3.1-8b-instruct';

  try {
    const result = await env.AI.run(model, {
      messages,
      temperature: options.temperature || 0.7,
      max_tokens: options.max_tokens || 512,
    });

    return result;
  } catch (error) {
    console.error('[WorkersAI] Error:', error.message);
    throw error;
  }
}

/**
 * G·ªçi Workers AI v·ªõi streaming
 * @param {Object} env - Cloudflare env v·ªõi AI binding
 * @param {Array} messages - Messages array
 * @param {Object} options - Options
 * @returns {ReadableStream} SSE stream
 */
async function callWorkersAIStream(env, messages, options = {}) {
  const model = options.model || env.MODEL || '@cf/meta/llama-3.1-8b-instruct';

  const result = await env.AI.run(model, {
    messages,
    temperature: options.temperature || 0.7,
    max_tokens: options.max_tokens || 512,
    stream: true,
  });

  return result;
}

/**
 * Parse JSON t·ª´ LLM response (c√≥ fallback)
 * @param {string} text - Raw response text
 * @returns {Object} Parsed JSON ho·∫∑c fallback object
 */
function parseAIResponse(text) {
  if (!text) {
    return createFallbackResponse('Kh√¥ng c√≥ ph·∫£n h·ªìi');
  }

  // T√¨m JSON trong response
  const jsonMatch = text.match(/\{[\s\S]*\}/);
  if (jsonMatch) {
    try {
      const parsed = JSON.parse(jsonMatch[0]);
      // Validate required fields
      if (parsed.reply && typeof parsed.reply === 'string') {
        return {
          riskLevel: parsed.riskLevel || 'green',
          emotion: parsed.emotion || '',
          reply: parsed.reply,
          nextQuestion: parsed.nextQuestion || '',
          actions: Array.isArray(parsed.actions) ? parsed.actions.slice(0, 4) : [],
          confidence: typeof parsed.confidence === 'number' ? parsed.confidence : 0.7,
          disclaimer: parsed.disclaimer || null,
        };
      }
    } catch (e) {
      console.error('[ParseAI] JSON parse error:', e.message);
    }
  }

  // Fallback: treat entire text as reply
  return createFallbackResponse(text);
}

function createFallbackResponse(text) {
  return {
    riskLevel: 'green',
    emotion: '',
    reply: text.slice(0, 500) || 'M√¨nh ƒëang l·∫Øng nghe b·∫°n. B·∫°n c√≥ th·ªÉ chia s·∫ª th√™m kh√¥ng?',
    nextQuestion: '',
    actions: [],
    confidence: 0.5,
    disclaimer: null,
  };
}

/**
 * 2-pass accuracy check khi confidence th·∫•p
 * @param {Object} env 
 * @param {Object} firstResponse 
 * @param {string} userMessage 
 * @returns {Promise<Object>} Verified response
 */
async function twoPassCheck(env, firstResponse, userMessage) {
  // N·∫øu confidence ƒë·ªß cao, kh√¥ng c·∫ßn pass 2
  if (firstResponse.confidence >= 0.6) {
    return firstResponse;
  }

  console.log('[2-Pass] Confidence th·∫•p, th·ª±c hi·ªán self-check...');

  const checkPrompt = `Ki·ªÉm tra l·∫°i c√¢u tr·∫£ l·ªùi sau v√† s·ª≠a n·∫øu c·∫ßn:

C√ÇU H·ªéI G·ªêC: ${userMessage}

C√ÇU TR·∫¢ L·ªúI DRAFT:
${JSON.stringify(firstResponse, null, 2)}

KI·ªÇM TRA:
1. C√≥ b·ªãa ƒë·∫∑t th√¥ng tin kh√¥ng?
2. C√≥ r·ªßi ro an to√†n kh√¥ng?
3. C√≥ ph√π h·ª£p v·ªõi SOS tier (${firstResponse.riskLevel}) kh√¥ng?
4. Gi·ªçng ƒëi·ªáu c√≥ th·∫•u c·∫£m kh√¥ng?

N·∫øu c·∫ßn s·ª≠a, tr·∫£ v·ªÅ JSON ho√†n ch·ªânh. N·∫øu kh√¥ng c·∫ßn s·ª≠a, tr·∫£ v·ªÅ JSON g·ªëc v·ªõi confidence cao h∆°n.`;

  try {
    const checkResult = await callWorkersAI(env, [
      { role: 'system', content: 'B·∫°n l√† chuy√™n gia ki·ªÉm tra ch·∫•t l∆∞·ª£ng ph·∫£n h·ªìi t√¢m l√Ω. Tr·∫£ v·ªÅ JSON.' },
      { role: 'user', content: checkPrompt }
    ], { temperature: 0.3 });

    const verified = parseAIResponse(checkResult.response || '');
    // Ensure confidence is updated
    if (verified.confidence < 0.6) verified.confidence = 0.65;
    return verified;
  } catch (e) {
    console.error('[2-Pass] Error:', e.message);
    // Fallback to first response
    firstResponse.confidence = 0.55;
    return firstResponse;
  }
}

// ============================================================================
// MAIN HANDLER
// ============================================================================
export default {
  async fetch(request, env) {
    // T·∫°o trace context cho observability
    const trace = createTraceContext(request, env);
    const startTime = Date.now();
    const origin = getAllowedOrigin(request, env);

    // CORS preflight
    if (request.method === 'OPTIONS') return handleOptions(request, env);

    // Only POST allowed
    if (request.method !== 'POST') {
      trace.logResponse(405);
      return addTraceHeader(json({ error: 'method_not_allowed' }, 405, origin), trace.traceId);
    }

    // Parse body
    let body;
    try {
      body = await request.json();
    } catch (_) {
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'invalid_json' }, 400, origin), trace.traceId);
    }

    const { message, history = [], memorySummary = '', userId = null, userName = null } = body || {};

    // Validate message
    if (!message || typeof message !== 'string') {
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'missing_message' }, 400, origin), trace.traceId);
    }

    // Sanitize input
    let sanitizedMessage;
    try {
      sanitizedMessage = sanitizeInput(message);
    } catch (e) {
      trace.log('warn', 'input_sanitized', { reason: e.message });
      trace.logResponse(400);
      return addTraceHeader(json({ error: 'invalid_input', reason: e.message }, 400, origin), trace.traceId);
    }

    // ========================================================================
    // SOS CLASSIFICATION (RULES-FIRST) - Enhanced v·ªõi context-aware
    // ========================================================================
    const riskLevel = classifyRiskRules(sanitizedMessage, history);

    // Log request v·ªõi observability
    trace.log('info', 'chat_request', {
      risk_level: riskLevel,
      model: env.MODEL || '@cf/meta/llama-3.1-8b-instruct',
      history_count: history.length,
      has_memory_summary: !!memorySummary,
    });

    // Real-time monitoring: Log SOS events v·ªõi structured logging
    if (riskLevel === 'red' || riskLevel === 'yellow') {
      trace.log('warn', 'sos_detected', {
        risk_level: riskLevel,
        message_length: sanitizedMessage.length,
        history_count: history.length,
        // Kh√¥ng log raw message ƒë·ªÉ b·∫£o v·ªá privacy
      });

      // C√≥ th·ªÉ g·ª≠i alert ƒë·∫øn admin n·∫øu c·∫ßn (future enhancement)
      // await sendAdminAlert(env, { riskLevel, traceId: trace.traceId });
    }

    // RED tier: tr·∫£ response chu·∫©n, kh√¥ng g·ªçi LLM
    if (riskLevel === 'red') {
      const redResponse = getRedTierResponse();
      trace.logResponse(200, { risk_level: 'red', sos: true });

      // Check if streaming requested - emit SSE format
      const url = new URL(request.url);
      const wantsStream = url.searchParams.get('stream') === 'true' ||
        request.headers.get('Accept')?.includes('text/event-stream');

      if (wantsStream) {
        // Emit RED tier response as SSE stream
        const stream = new ReadableStream({
          start(controller) {
            const enc = new TextEncoder();
            const send = (line) => controller.enqueue(enc.encode(line));

            // Meta event
            send(`event: meta\n`);
            send(`data: ${JSON.stringify({ trace_id: trace.traceId, riskLevel: 'red', sos: true })}\n\n`);

            // Send the reply text
            const replyText = redResponse.reply + '\n\nüìû ' + redResponse.actions.join('\nüìû ');
            send(`data: ${JSON.stringify({ type: 'delta', text: replyText })}\n\n`);

            // Done event
            send(`data: ${JSON.stringify({ type: 'done' })}\n\n`);

            controller.close();
          },
        });
        return new Response(stream, { status: 200, headers: sseHeaders(origin, trace.traceId) });
      }

      return addTraceHeader(json(redResponse, 200, origin), trace.traceId);
    }


    // ========================================================================
    // CHECK TOKEN LIMIT
    // ========================================================================
    const tokenCheck = await checkTokenLimit(env);
    if (!tokenCheck.allowed) {
      trace.log('warn', 'token_limit_exceeded', {
        tokens: tokenCheck.tokens,
        limit: tokenCheck.limit,
      });
      trace.logResponse(429);
      return addTraceHeader(json({
        error: 'token_limit_exceeded',
        message: 'ƒê√£ ƒë·∫°t gi·ªõi h·∫°n s·ª≠ d·ª•ng th√°ng n√†y. Vui l√≤ng th·ª≠ l·∫°i v√†o th√°ng sau.',
        tokens: tokenCheck.tokens,
        limit: tokenCheck.limit,
      }, 429, origin), trace.traceId);
    }

    // ========================================================================
    // RAG: Retrieve relevant context t·ª´ knowledge base
    // ========================================================================
    let ragContext = '';
    let usedRAG = 0;
    try {
      // Query knowledge base t·ª´ D1
      const kbResult = await env.ban_dong_hanh_db.prepare(
        `SELECT id, title, content, category, tags FROM knowledge_base 
         WHERE content LIKE ? OR title LIKE ? OR tags LIKE ?
         LIMIT 20`
      ).bind(
        `%${sanitizedMessage.slice(0, 50)}%`, // Search trong content
        `%${sanitizedMessage.slice(0, 30)}%`, // Search trong title
        `%${sanitizedMessage.slice(0, 30)}%`  // Search trong tags
      ).all().catch(() => ({ results: [] }));

      const knowledgeBase = kbResult.results.map(doc => ({
        id: doc.id,
        content: `${doc.title}\n${doc.content}`,
        category: doc.category,
        source: 'knowledge_base',
        tags: doc.tags ? JSON.parse(doc.tags) : [],
        // Note: embedding s·∫Ω ƒë∆∞·ª£c load t·ª´ DB trong hybridSearch n·∫øu c√≥
      }));

      if (knowledgeBase.length > 0) {
        // Import RAG functions
        const { hybridSearch, formatRAGContext } = await import('./rag.js');

        const retrievedDocs = await hybridSearch(
          sanitizedMessage,
          knowledgeBase,
          env,
          { topK: 3, bm25Weight: 0.6, denseWeight: 0.4 }
        ).catch(async () => {
          // Fallback to BM25 only n·∫øu hybrid search fail
          const { bm25Search } = await import('./rag.js');
          return bm25Search(sanitizedMessage, knowledgeBase).slice(0, 3);
        });

        if (retrievedDocs && retrievedDocs.length > 0) {
          const { formatRAGContext } = await import('./rag.js');
          ragContext = formatRAGContext(retrievedDocs);
          usedRAG = 1;
          trace.log('info', 'rag_used', {
            docs_retrieved: retrievedDocs.length,
            categories: retrievedDocs.map(d => d.category).join(',')
          });
        }
      }
    } catch (error) {
      // RAG l√† optional, kh√¥ng block n·∫øu l·ªói
      trace.log('warn', 'rag_retrieval_failed', { error: error.message });
    }

    // ========================================================================
    // LOAD USER MEMORY (Persistent context cho t·ª´ng user)
    // ========================================================================
    let userMemory = null;
    let userMemoryContext = 'ƒê√¢y l√† l·∫ßn ƒë·∫ßu ti√™n g·∫∑p user n√†y.';

    if (userId) {
      try {
        userMemory = await loadUserMemory(env, userId);
        userMemoryContext = formatMemoryContext(userMemory);
        trace.log('info', 'user_memory_loaded', {
          user_id: userId,
          trust_level: userMemory?.trustLevel || 'new',
          total_conversations: userMemory?.totalConversations || 0
        });
      } catch (error) {
        trace.log('warn', 'user_memory_load_failed', { error: error.message });
        // Continue without memory - fallback to stateless
      }
    }

    // Explicitly add userName if provided from frontend
    if (userName) {
      userMemoryContext = `T√™n c·ªßa user l√†: ${userName}.\n` + userMemoryContext;
    }

    // ========================================================================
    // PREPARE MESSAGES FOR LLM (v·ªõi RAG context + User Memory)
    // ========================================================================
    // Inject user memory v√†o system prompt
    let systemPromptWithContext = SYSTEM_INSTRUCTIONS.replace(
      '[USER_MEMORY_CONTEXT]',
      userMemoryContext
    );

    // Th√™m RAG context v√†o system prompt n·∫øu c√≥
    if (ragContext) {
      systemPromptWithContext = systemPromptWithContext + ragContext;
    }

    const messages = formatMessagesForLLM(
      systemPromptWithContext,
      getRecentMessages(history, 8),
      sanitizedMessage,
      memorySummary
    );

    // Estimate tokens for this request (c·∫£i thi·ªán accuracy)
    const estimatedTokens = countTokensAccurate(
      JSON.stringify(messages) + sanitizedMessage,
      env.MODEL || '@cf/meta/llama-3.1-8b-instruct'
    );

    // ========================================================================
    // CHECK IF STREAMING REQUESTED
    // ========================================================================
    const url = new URL(request.url);
    const wantsStream = url.searchParams.get('stream') === 'true' ||
      request.headers.get('Accept')?.includes('text/event-stream');

    try {
      if (wantsStream) {
        // ====================================================================
        // STREAMING RESPONSE
        // ====================================================================
        const aiStream = await callWorkersAIStream(env, messages);

        const stream = new ReadableStream({
          async start(controller) {
            const enc = new TextEncoder();
            const send = (line) => controller.enqueue(enc.encode(line));

            // Send meta event
            send(`event: meta\n`);
            send(`data: ${JSON.stringify({ trace_id: trace.traceId, riskLevel })}\n\n`);

            try {
              let fullText = '';
              const reader = aiStream.getReader();
              let buffer = '';

              while (true) {
                const { value, done } = await reader.read();
                if (done) break;

                // Workers AI stream returns chunks as SSE-like format
                const chunk = typeof value === 'string' ? value : new TextDecoder().decode(value);
                buffer += chunk;

                // Parse SSE lines from buffer
                let lineEnd;
                while ((lineEnd = buffer.indexOf('\n')) !== -1) {
                  const line = buffer.slice(0, lineEnd).trim();
                  buffer = buffer.slice(lineEnd + 1);

                  if (line.startsWith('data: ')) {
                    const dataStr = line.slice(6);
                    if (dataStr === '[DONE]') {
                      continue;
                    }
                    try {
                      const parsed = JSON.parse(dataStr);
                      // Workers AI tr·∫£ v·ªÅ {"response":"text"} ho·∫∑c {"response":null} khi done
                      if (parsed.response && typeof parsed.response === 'string') {
                        fullText += parsed.response;
                        // Send delta event v·ªõi format chu·∫©n cho frontend
                        send(`data: ${JSON.stringify({ type: 'delta', text: parsed.response })}\n\n`);
                      }
                    } catch (_) {
                      // Skip non-JSON lines
                    }
                  }
                }
              }

              // Send done event
              send(`data: ${JSON.stringify({ type: 'done' })}\n\n`);

              // Update token usage (estimate t·ª´ full text)
              const responseTokens = estimateTokens(fullText);
              const totalTokens = estimatedTokens + responseTokens;
              const tokenUsageResult = await addTokenUsage(env, totalTokens);

              // T√≠nh cost (∆∞·ªõc t√≠nh: $0.0005 per 1k tokens cho llama-3.1-8b)
              const costUsd = (totalTokens / 1000) * 0.0005;
              const model = env.MODEL || '@cf/meta/llama-3.1-8b-instruct';
              const modelVersion = model.split('@cf/')[1] || 'llama-3.1-8b-instruct';

              // Log model call v·ªõi tokens v√† cost
              const streamLatencyMs = Date.now() - startTime;
              trace.logModelCall(model, modelVersion, estimatedTokens, responseTokens, costUsd, streamLatencyMs);

              // Log completion
              trace.logResponse(200, {
                risk_level: riskLevel,
                tokens_in: estimatedTokens,
                tokens_out: responseTokens,
                tokens_total: totalTokens,
                cost_usd: costUsd,
                stream: true,
                token_usage: tokenUsageResult.tokens,
                token_warning: tokenUsageResult.warning,
              });

              // Log streaming response v√†o chat_responses (sau khi stream complete)
              // Note: V·ªõi streaming, ch√∫ng ta log sau khi c√≥ full response
              // ƒê·ªÉ ƒë∆°n gi·∫£n, log v·ªõi partial response v√† update sau n·∫øu c·∫ßn
              try {
                await env.ban_dong_hanh_db.prepare(
                  `INSERT INTO chat_responses 
                   (user_id, message_id, user_message, ai_response, risk_level, confidence, tokens_used, latency_ms, used_rag, prompt_version)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)`
                ).bind(
                  userId || null,
                  trace.traceId,
                  sanitizedMessage.slice(0, 1000),
                  '[STREAMING]', // Placeholder, c√≥ th·ªÉ update sau
                  riskLevel || 'green',
                  0.8, // Default confidence
                  totalTokens,
                  streamLatencyMs,
                  usedRAG,
                  PROMPT_VERSION
                ).run().catch(err => {
                  trace.log('warn', 'stream_response_log_failed', { error: err.message });
                });
              } catch (err) {
                trace.log('warn', 'stream_response_log_error', { error: err.message });
              }

            } catch (err) {
              trace.logError(err, { stream: true });
              const errPayload = { type: 'error', error: 'model_error', note: String(err?.message || err) };
              send(`data: ${JSON.stringify(errPayload)}\n\n`);
            } finally {
              controller.close();
            }
          },
        });

        return new Response(stream, { status: 200, headers: sseHeaders(origin, trace.traceId) });

      } else {
        // ====================================================================
        // NON-STREAMING RESPONSE (v·ªõi 2-pass check)
        // ====================================================================
        const result = await callWorkersAI(env, messages);
        const rawResponse = result.response || '';

        // Parse response
        let parsed = parseAIResponse(rawResponse);

        // Override riskLevel t·ª´ rules n·∫øu kh√°c
        if (riskLevel === 'yellow' && parsed.riskLevel === 'green') {
          parsed.riskLevel = 'yellow';
        }

        // 2-pass accuracy check
        parsed = await twoPassCheck(env, parsed, sanitizedMessage);

        // Update token usage (estimate t·ª´ response)
        const responseTokens = estimateTokens(parsed.reply || '');
        const totalTokens = estimatedTokens + responseTokens;
        const tokenUsageResult = await addTokenUsage(env, totalTokens);

        // T√≠nh cost (∆∞·ªõc t√≠nh: $0.0005 per 1k tokens cho llama-3.1-8b)
        const costUsd = (totalTokens / 1000) * 0.0005;
        const model = env.MODEL || '@cf/meta/llama-3.1-8b-instruct';
        const modelVersion = model.split('@cf/')[1] || 'llama-3.1-8b-instruct';

        // Log model call v·ªõi tokens v√† cost
        trace.logModelCall(model, modelVersion, estimatedTokens, responseTokens, costUsd, Date.now() - startTime);

        // Log completion
        trace.logResponse(200, {
          risk_level: parsed.riskLevel,
          confidence: parsed.confidence,
          tokens_in: estimatedTokens,
          tokens_out: responseTokens,
          tokens_total: totalTokens,
          cost_usd: costUsd,
          stream: false,
          token_usage: tokenUsageResult.tokens,
          token_warning: tokenUsageResult.warning,
        });

        // ================================================================
        // UPDATE USER MEMORY (sau khi c√≥ response t·ª´ AI)
        // ================================================================
        if (userId && parsed.memoryUpdate) {
          try {
            await updateUserMemory(env, userId, parsed.memoryUpdate, sanitizedMessage, trace.traceId);
            trace.log('info', 'user_memory_updated', {
              user_id: userId,
              new_facts_count: parsed.memoryUpdate?.newFacts?.length || 0,
              emotion: parsed.memoryUpdate?.emotionPattern || null
            });
          } catch (error) {
            trace.log('warn', 'user_memory_update_failed', { error: error.message });
            // Non-blocking - continue to return response
          }
        }

        // Remove memoryUpdate from response (internal only)
        const { memoryUpdate, ...responseWithoutMemory } = parsed;

        return addTraceHeader(json(responseWithoutMemory, 200, origin), trace.traceId);
      }

    } catch (e) {
      trace.logError(e, { route: 'ai:chat' });
      trace.logResponse(502);
      return addTraceHeader(json({ error: 'model_error', note: String(e?.message || e) }, 502, origin), trace.traceId);
    }
  },
};
